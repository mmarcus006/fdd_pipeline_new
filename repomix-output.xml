<repomix><file_summary>This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.
The content has been processed where empty lines have been removed, line numbers have been added, content has been formatted for parsing in xml style, content has been compressed (code blocks are separated by ⋮---- delimiter).<purpose>This file contains a packed representation of a subset of the repository&apos;s contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.</purpose><file_format>The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file</file_format><usage_guidelines>- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.</usage_guidelines><notes>- Some files may have been excluded based on .gitignore rules and Repomix&apos;s configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: docs
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Line numbers have been added to the beginning of each line
- Content has been formatted for parsing in xml style
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)</notes></file_summary><directory_structure>docs/API_REFERENCE.md
docs/ARCHITECTURE.md
docs/data_flow.md
docs/database_schema.md
docs/MINERU_INTEGRATION.md
docs/mineru_processing.md
docs/pydantic_models.md
docs/system_overview.md
docs/TECH_STACK.md
docs/troubleshooting.md
docs/validation_rules.md</directory_structure><files>This section contains the contents of the repository&apos;s files.<file path="docs/API_REFERENCE.md"># API Reference

## Overview

The FDD Pipeline provides a REST API built with FastAPI for programmatic access to document processing and data retrieval functionality. The API is designed for internal use and integration with other systems.

## Base URL

```
http://localhost:8000
```

## Authentication

Currently, the API does not require authentication as it&apos;s designed for internal use. Authentication will be added in future versions.

## Endpoints

### Health Check

Check if the API is running and healthy.

```http
GET /health
```

**Response:**
```json
{
  &quot;status&quot;: &quot;healthy&quot;,
  &quot;timestamp&quot;: &quot;2024-01-15T10:30:00Z&quot;,
  &quot;version&quot;: &quot;1.0.0&quot;
}
```

### Process PDF

Submit a PDF for processing through the complete pipeline.

```http
POST /process-pdf
```

**Request Body:**
```json
{
  &quot;pdf_url&quot;: &quot;https://example.com/path/to/fdd.pdf&quot;,
  &quot;franchise_name&quot;: &quot;Example Franchise LLC&quot;,  // Optional
  &quot;metadata&quot;: {
    &quot;source&quot;: &quot;manual_upload&quot;,
    &quot;filing_date&quot;: &quot;2024-01-15&quot;
  }
}
```

**Response:**
```json
{
  &quot;task_id&quot;: &quot;550e8400-e29b-41d4-a716-446655440000&quot;,
  &quot;status&quot;: &quot;processing&quot;,
  &quot;message&quot;: &quot;PDF processing started&quot;,
  &quot;estimated_time_seconds&quot;: 300
}
```

### Get Task Status

Check the status of a processing task.

```http
GET /task/{task_id}
```

**Response:**
```json
{
  &quot;task_id&quot;: &quot;550e8400-e29b-41d4-a716-446655440000&quot;,
  &quot;status&quot;: &quot;completed&quot;,  // pending, processing, completed, failed
  &quot;progress&quot;: 100,
  &quot;result&quot;: {
    &quot;fdd_id&quot;: &quot;660e8400-e29b-41d4-a716-446655440001&quot;,
    &quot;franchise_name&quot;: &quot;Example Franchise LLC&quot;,
    &quot;sections_processed&quot;: 23,
    &quot;extraction_complete&quot;: true
  },
  &quot;error&quot;: null,
  &quot;created_at&quot;: &quot;2024-01-15T10:30:00Z&quot;,
  &quot;completed_at&quot;: &quot;2024-01-15T10:35:00Z&quot;
}
```

### Search Franchises

Search for franchises in the database.

```http
GET /franchises/search?q={query}&amp;limit={limit}&amp;offset={offset}
```

**Query Parameters:**
- `q` (required): Search query
- `limit` (optional): Number of results (default: 20, max: 100)
- `offset` (optional): Pagination offset (default: 0)

**Response:**
```json
{
  &quot;total&quot;: 150,
  &quot;results&quot;: [
    {
      &quot;id&quot;: &quot;550e8400-e29b-41d4-a716-446655440000&quot;,
      &quot;canonical_name&quot;: &quot;Example Franchise LLC&quot;,
      &quot;parent_company&quot;: &quot;Example Corp&quot;,
      &quot;website&quot;: &quot;https://example.com&quot;,
      &quot;latest_fdd&quot;: {
        &quot;id&quot;: &quot;660e8400-e29b-41d4-a716-446655440001&quot;,
        &quot;issue_date&quot;: &quot;2024-01-01&quot;,
        &quot;filing_state&quot;: &quot;MN&quot;
      }
    }
  ],
  &quot;limit&quot;: 20,
  &quot;offset&quot;: 0
}
```

### Get Franchise Details

Get detailed information about a specific franchise.

```http
GET /franchises/{franchise_id}
```

**Response:**
```json
{
  &quot;id&quot;: &quot;550e8400-e29b-41d4-a716-446655440000&quot;,
  &quot;canonical_name&quot;: &quot;Example Franchise LLC&quot;,
  &quot;parent_company&quot;: &quot;Example Corp&quot;,
  &quot;website&quot;: &quot;https://example.com&quot;,
  &quot;phone&quot;: &quot;+1-555-123-4567&quot;,
  &quot;email&quot;: &quot;franchise@example.com&quot;,
  &quot;address&quot;: {
    &quot;street&quot;: &quot;123 Main St&quot;,
    &quot;city&quot;: &quot;Minneapolis&quot;,
    &quot;state&quot;: &quot;MN&quot;,
    &quot;zip&quot;: &quot;55401&quot;
  },
  &quot;dba_names&quot;: [&quot;Example Franchise&quot;, &quot;EF&quot;],
  &quot;fdds&quot;: [
    {
      &quot;id&quot;: &quot;660e8400-e29b-41d4-a716-446655440001&quot;,
      &quot;issue_date&quot;: &quot;2024-01-01&quot;,
      &quot;document_type&quot;: &quot;Initial&quot;,
      &quot;filing_state&quot;: &quot;MN&quot;,
      &quot;processing_status&quot;: &quot;completed&quot;
    }
  ],
  &quot;created_at&quot;: &quot;2023-01-15T10:30:00Z&quot;,
  &quot;updated_at&quot;: &quot;2024-01-15T10:30:00Z&quot;
}
```

### Get FDD Data

Retrieve extracted data from a specific FDD.

```http
GET /fdds/{fdd_id}/data
```

**Query Parameters:**
- `items` (optional): Comma-separated list of items to include (e.g., &quot;5,6,7,19,20,21&quot;)

**Response:**
```json
{
  &quot;fdd_id&quot;: &quot;660e8400-e29b-41d4-a716-446655440001&quot;,
  &quot;franchise_name&quot;: &quot;Example Franchise LLC&quot;,
  &quot;issue_date&quot;: &quot;2024-01-01&quot;,
  &quot;data&quot;: {
    &quot;item5&quot;: {
      &quot;initial_franchise_fee&quot;: {
        &quot;amount_cents&quot;: 4500000,
        &quot;refundable&quot;: false,
        &quot;due_at&quot;: &quot;Signing&quot;
      }
    },
    &quot;item6&quot;: {
      &quot;royalty_fee&quot;: {
        &quot;amount_percentage&quot;: 6.0,
        &quot;frequency&quot;: &quot;Monthly&quot;,
        &quot;calculation_basis&quot;: &quot;Gross Sales&quot;
      },
      &quot;marketing_fee&quot;: {
        &quot;amount_percentage&quot;: 2.0,
        &quot;frequency&quot;: &quot;Monthly&quot;,
        &quot;calculation_basis&quot;: &quot;Gross Sales&quot;
      }
    },
    &quot;item7&quot;: {
      &quot;total_investment&quot;: {
        &quot;low_cents&quot;: 15000000,
        &quot;high_cents&quot;: 35000000
      },
      &quot;categories&quot;: [
        {
          &quot;category&quot;: &quot;Real Estate&quot;,
          &quot;low_cents&quot;: 5000000,
          &quot;high_cents&quot;: 15000000
        }
      ]
    }
  }
}
```

### Trigger State Scraping

Manually trigger scraping for a specific state portal.

```http
POST /scrape/{state}
```

**Path Parameters:**
- `state`: State code (minnesota, wisconsin)

**Request Body:**
```json
{
  &quot;limit&quot;: 10,  // Optional: limit number of documents
  &quot;download&quot;: true  // Optional: whether to download PDFs
}
```

**Response:**
```json
{
  &quot;task_id&quot;: &quot;770e8400-e29b-41d4-a716-446655440000&quot;,
  &quot;state&quot;: &quot;minnesota&quot;,
  &quot;status&quot;: &quot;started&quot;,
  &quot;message&quot;: &quot;Scraping task initiated&quot;
}
```

### Get Extraction Metrics

Retrieve metrics about extraction performance.

```http
GET /metrics/extractions
```

**Query Parameters:**
- `start_date` (optional): ISO date string
- `end_date` (optional): ISO date string
- `model` (optional): Filter by LLM model

**Response:**
```json
{
  &quot;total_extractions&quot;: 1250,
  &quot;success_rate&quot;: 0.96,
  &quot;average_time_seconds&quot;: 45.2,
  &quot;by_model&quot;: {
    &quot;gemini-pro&quot;: {
      &quot;count&quot;: 800,
      &quot;success_rate&quot;: 0.98,
      &quot;average_time&quot;: 35.5
    },
    &quot;gpt-4&quot;: {
      &quot;count&quot;: 350,
      &quot;success_rate&quot;: 0.95,
      &quot;average_time&quot;: 58.3
    },
    &quot;ollama:llama3&quot;: {
      &quot;count&quot;: 100,
      &quot;success_rate&quot;: 0.88,
      &quot;average_time&quot;: 42.1
    }
  },
  &quot;by_item&quot;: {
    &quot;5&quot;: {&quot;count&quot;: 208, &quot;success_rate&quot;: 0.99},
    &quot;6&quot;: {&quot;count&quot;: 208, &quot;success_rate&quot;: 0.97},
    &quot;7&quot;: {&quot;count&quot;: 208, &quot;success_rate&quot;: 0.96},
    &quot;19&quot;: {&quot;count&quot;: 208, &quot;success_rate&quot;: 0.94},
    &quot;20&quot;: {&quot;count&quot;: 208, &quot;success_rate&quot;: 0.95},
    &quot;21&quot;: {&quot;count&quot;: 208, &quot;success_rate&quot;: 0.93}
  }
}
```

## Error Responses

All endpoints return consistent error responses:

```json
{
  &quot;error&quot;: {
    &quot;code&quot;: &quot;VALIDATION_ERROR&quot;,
    &quot;message&quot;: &quot;Invalid request parameters&quot;,
    &quot;details&quot;: {
      &quot;field&quot;: &quot;pdf_url&quot;,
      &quot;reason&quot;: &quot;Invalid URL format&quot;
    }
  },
  &quot;request_id&quot;: &quot;req_123456789&quot;
}
```

### Error Codes

| Code | Description |
|------|-------------|
| `VALIDATION_ERROR` | Invalid request parameters |
| `NOT_FOUND` | Resource not found |
| `PROCESSING_ERROR` | Error during processing |
| `RATE_LIMIT` | Too many requests |
| `INTERNAL_ERROR` | Internal server error |

## Rate Limiting

The API implements rate limiting to prevent abuse:

- **Default limit**: 100 requests per minute per IP
- **Burst limit**: 10 concurrent requests
- **Headers returned**:
  - `X-RateLimit-Limit`: Request limit
  - `X-RateLimit-Remaining`: Remaining requests
  - `X-RateLimit-Reset`: Reset timestamp

## WebSocket Support (Future)

WebSocket support for real-time updates is planned for future versions:

```
ws://localhost:8000/ws/task/{task_id}
```

## OpenAPI Documentation

Interactive API documentation is available at:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **OpenAPI Schema**: http://localhost:8000/openapi.json

## SDK Support

Python SDK example:

```python
from fdd_pipeline_sdk import FDDClient

client = FDDClient(base_url=&quot;http://localhost:8000&quot;)

# Process a PDF
task = client.process_pdf(
    pdf_url=&quot;https://example.com/fdd.pdf&quot;,
    franchise_name=&quot;Example Franchise&quot;
)

# Check status
status = client.get_task_status(task.id)

# Search franchises
results = client.search_franchises(&quot;pizza&quot;, limit=10)

# Get FDD data
data = client.get_fdd_data(fdd_id, items=[5, 6, 7])
```

## Best Practices

1. **Use pagination** for search endpoints to avoid large responses
2. **Poll task status** with exponential backoff for long-running operations
3. **Handle rate limits** gracefully with retry logic
4. **Cache responses** when appropriate to reduce API calls
5. **Use specific item filters** when retrieving FDD data to minimize payload size

## Versioning

The API uses URL versioning. The current version is v1 (implicit). Future versions will use:

```
http://localhost:8000/v2/franchises/search
```

## Changelog

### v1.0.0 (Current)
- Initial API release
- Basic CRUD operations
- PDF processing endpoints
- Search functionality
- Metrics endpoints</file><file path="docs/MINERU_INTEGRATION.md"># MinerU Web API Integration Guide

## Overview

MinerU is a state-of-the-art PDF processing service that provides advanced layout analysis, table detection, and content extraction capabilities. The FDD Pipeline integrates with MinerU&apos;s Web API to process complex PDF documents before LLM extraction.

## How It Works

### Architecture

```mermaid
sequenceDiagram
    participant Pipeline
    participant MinerU Client
    participant Browser
    participant MinerU API
    
    Pipeline-&gt;&gt;MinerU Client: Process PDF
    MinerU Client-&gt;&gt;Browser: Launch authentication
    Browser-&gt;&gt;MinerU API: GitHub OAuth login
    MinerU API-&gt;&gt;Browser: Auth token
    Browser-&gt;&gt;MinerU Client: Save auth
    MinerU Client-&gt;&gt;MinerU API: Submit PDF URL
    MinerU API-&gt;&gt;MinerU Client: Task ID
    loop Poll status
        MinerU Client-&gt;&gt;MinerU API: Check task
    end
    MinerU API-&gt;&gt;MinerU Client: Download results
    MinerU Client-&gt;&gt;Pipeline: Return structured data
```

### Authentication Flow

MinerU uses GitHub OAuth for authentication. The integration handles this automatically:

1. **First Run**: Opens browser for GitHub login
2. **Subsequent Runs**: Uses saved authentication from `mineru_auth.json`
3. **Token Refresh**: Automatically handled when expired

## Setup

### 1. Install Dependencies

```bash
# Install Playwright for browser automation
playwright install chromium

# Python dependencies are included in main requirements
uv sync
```

### 2. Configuration

The MinerU client is configured through environment variables:

```bash
# In .env file
MINERU_AUTH_FILE=mineru_auth.json  # Where to store auth credentials
```

### 3. First-Time Authentication

When running for the first time:

```python
from src.MinerU.mineru_web_api import MinerUAPI

# Create client
api = MinerUAPI()

# Login - will open browser
api.login(use_saved=False)  # Force new login
```

## Usage

### Basic PDF Processing

```python
from src.MinerU.mineru_web_api import MinerUAPI

# Initialize client
api = MinerUAPI()

# Login (uses saved auth if available)
api.login(use_saved=True)

# Process a PDF
pdf_url = &quot;https://example.com/path/to/fdd.pdf&quot;
results = api.process_pdf(pdf_url, wait_time=300)

# Results contain:
# - markdown: Extracted text in markdown format
# - json: Structured layout information
# - filename: Original filename
```

### Integration with Pipeline

The pipeline automatically uses MinerU through the `mineru_processing` task:

```python
from tasks.mineru_processing import process_with_mineru

# Process PDF and get structured data
mineru_data = await process_with_mineru(
    pdf_path=&quot;/path/to/local.pdf&quot;,
    drive_file_id=&quot;google_drive_id&quot;
)
```

## Output Format

### Markdown Output

The markdown file contains:
- Extracted text with formatting preserved
- Table structures converted to markdown tables
- Headers and sections identified
- Page breaks marked

Example:
```markdown
# FRANCHISE DISCLOSURE DOCUMENT

## Item 1: THE FRANCHISOR AND ANY PARENTS, PREDECESSORS AND AFFILIATES

The franchisor is Example Franchise LLC...

### Table 1: Initial Investment

| Type | Amount | When Due |
|------|--------|----------|
| Initial Fee | $45,000 | At signing |
| Equipment | $50,000-$75,000 | Before opening |
```

### JSON Output

The JSON file contains detailed layout information:

```json
{
  &quot;pages&quot;: [
    {
      &quot;page_no&quot;: 1,
      &quot;blocks&quot;: [
        {
          &quot;type&quot;: &quot;title&quot;,
          &quot;text&quot;: &quot;FRANCHISE DISCLOSURE DOCUMENT&quot;,
          &quot;bbox&quot;: [100, 100, 500, 150]
        },
        {
          &quot;type&quot;: &quot;table&quot;,
          &quot;cells&quot;: [...],
          &quot;bbox&quot;: [100, 200, 500, 400]
        }
      ]
    }
  ],
  &quot;metadata&quot;: {
    &quot;total_pages&quot;: 250,
    &quot;extracted_at&quot;: &quot;2024-01-15T10:30:00Z&quot;
  }
}
```

## Error Handling

### Common Issues and Solutions

1. **Authentication Failures**
   ```python
   # Delete saved auth and re-authenticate
   import os
   os.remove(&quot;mineru_auth.json&quot;)
   api.login(use_saved=False)
   ```

2. **Rate Limiting**
   - MinerU has rate limits on the free tier
   - The client automatically handles 429 errors with backoff
   - Consider upgrading to paid tier for production use

3. **Processing Timeouts**
   ```python
   # Increase wait time for large documents
   results = api.process_pdf(pdf_url, wait_time=600)  # 10 minutes
   ```

4. **Network Issues**
   - The client includes retry logic for transient failures
   - Check proxy settings if behind corporate firewall

## Advanced Configuration

### Custom Headers

```python
# Add custom headers if needed
api.session.headers.update({
    &quot;X-Custom-Header&quot;: &quot;value&quot;
})
```

### Proxy Support

```python
# Configure proxy for corporate environments
api.session.proxies = {
    &quot;http&quot;: &quot;http://proxy.company.com:8080&quot;,
    &quot;https&quot;: &quot;http://proxy.company.com:8080&quot;
}
```

### Debug Mode

Enable debug logging:

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Or set in environment
DEBUG=true python main.py
```

## Performance Optimization

### Caching

The client includes a 15-minute cache for processed documents:

```python
# Process same PDF multiple times - uses cache
results1 = api.process_pdf(pdf_url)  # Processes
results2 = api.process_pdf(pdf_url)  # From cache
```

### Batch Processing

For multiple PDFs, process in parallel:

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def process_batch(pdf_urls):
    with ThreadPoolExecutor(max_workers=3) as executor:
        tasks = [
            asyncio.get_event_loop().run_in_executor(
                executor, api.process_pdf, url
            )
            for url in pdf_urls
        ]
        results = await asyncio.gather(*tasks)
    return results
```

## Monitoring

### Logging

All MinerU operations are logged:

```
2024-01-15 10:30:00 INFO: Processing PDF: example.pdf
2024-01-15 10:30:05 INFO: Task ID: 123e4567-e89b-12d3-a456-426614174000
2024-01-15 10:30:15 INFO: Status: processing
2024-01-15 10:31:00 INFO: Status: done
2024-01-15 10:31:05 INFO: Downloaded: example.md (2.5MB)
```

### Metrics

Track processing metrics:

```python
from utils.extraction_monitoring import ExtractionMonitor

monitor = ExtractionMonitor()
monitor.record_mineru_processing(
    pdf_path=pdf_path,
    processing_time=elapsed_time,
    success=True,
    output_size=len(markdown_content)
)
```

## Troubleshooting

### Debug Checklist

1. **Check Authentication**
   ```bash
   # Verify auth file exists and is valid
   cat mineru_auth.json | jq &apos;.cookies[] | select(.name==&quot;uaa-token&quot;)&apos;
   ```

2. **Test Connection**
   ```python
   # Simple connectivity test
   response = api.session.get(&quot;https://mineru.net/api/v4/tasks&quot;)
   print(f&quot;Status: {response.status_code}&quot;)
   ```

3. **Verify Browser Installation**
   ```bash
   # Check Playwright browsers
   playwright show-browsers
   ```

### Common Error Messages

| Error | Cause | Solution |
|-------|-------|----------|
| &quot;Not logged in&quot; | Missing or expired auth | Re-authenticate with `api.login()` |
| &quot;Task timeout&quot; | Large PDF or slow processing | Increase `wait_time` parameter |
| &quot;Rate limit exceeded&quot; | Too many requests | Add delays or upgrade account |
| &quot;Browser launch failed&quot; | Missing Chromium | Run `playwright install chromium` |

## Best Practices

1. **Authentication Management**
   - Store `mineru_auth.json` securely
   - Don&apos;t commit auth file to version control
   - Refresh auth tokens periodically

2. **Error Handling**
   - Always wrap API calls in try-except blocks
   - Implement exponential backoff for retries
   - Log all errors for debugging

3. **Performance**
   - Process PDFs in batches when possible
   - Use caching for frequently accessed documents
   - Monitor rate limits and adjust accordingly

4. **Production Deployment**
   - Use headless browser mode
   - Set up proper logging and monitoring
   - Consider paid tier for higher limits

## API Limitations

### Free Tier
- Rate limit: 10 PDFs per hour
- Max file size: 50MB
- Processing timeout: 5 minutes
- No priority queue

### Paid Tiers
- Higher rate limits
- Larger file sizes
- Priority processing
- API-only authentication (no browser)

## Future Enhancements

1. **Direct API Integration**: Remove browser dependency when API keys are available
2. **Local Deployment**: Self-hosted MinerU option for sensitive documents
3. **Webhook Support**: Real-time notifications instead of polling
4. **Batch API**: Process multiple PDFs in single request</file><file path="docs/data_flow.md"># Data Flow Documentation

## Overview

This document details how data flows through the FDD Pipeline from initial acquisition to final storage. Each stage includes data transformations, validation points, and error handling mechanisms.

## End-to-End Data Flow

```mermaid
graph TB
    subgraph &quot;Stage 1: Acquisition&quot;
        A1[State Portal HTML] --&gt; A2[Scraper Extraction]
        A2 --&gt; A3[Filing Metadata]
        A3 --&gt; A4[PDF Download]
        A4 --&gt; A5[Google Drive Upload]
    end
    
    subgraph &quot;Stage 2: Registration&quot;
        A5 --&gt; B1[SHA256 Hash]
        B1 --&gt; B2[Duplicate Check]
        B2 --&gt; B3[Franchise Matching]
        B3 --&gt; B4[FDD Record Creation]
    end
    
    subgraph &quot;Stage 3: Segmentation&quot;
        B4 --&gt; C1[MinerU Processing]
        C1 --&gt; C2[Layout JSON]
        C2 --&gt; C3[Section Detection]
        C3 --&gt; C4[PDF Splitting]
        C4 --&gt; C5[Section Records]
    end
    
    subgraph &quot;Stage 4: Extraction&quot;
        C5 --&gt; D1[Section Prioritization]
        D1 --&gt; D2[Model Selection]
        D2 --&gt; D3[Prompt Generation]
        D3 --&gt; D4[LLM Extraction]
        D4 --&gt; D5[Schema Validation]
    end
    
    subgraph &quot;Stage 5: Storage&quot;
        D5 --&gt; E1[Type Routing]
        E1 --&gt; E2[Table Insertion]
        E2 --&gt; E3[Index Updates]
        E3 --&gt; E4[Cache Refresh]
    end
```

## Stage 1: Document Acquisition

### 1.1 Web Scraping Flow

**Minnesota Portal Flow**:
```python
# Simplified flow representation
async def scrape_minnesota():
    # 1. Fetch search page
    response = await fetch(&quot;https://mn.gov/commerce/fdd/search&quot;)
    
    # 2. Extract document links
    documents = parse_search_results(response.html)
    
    # 3. For each document
    for doc in documents:
        metadata = {
            &quot;source&quot;: &quot;MN&quot;,
            &quot;filing_number&quot;: doc.filing_id,
            &quot;filing_date&quot;: doc.date,
            &quot;franchise_name&quot;: doc.name,
            &quot;document_url&quot;: doc.pdf_url
        }
        
        # 4. Download PDF
        pdf_content = await download_pdf(doc.pdf_url)
        
        # 5. Upload to Drive
        file_id = await upload_to_drive(pdf_content, metadata)
        
        yield DocumentRecord(metadata, file_id)
```

**Wisconsin Portal Flow**:
```mermaid
sequenceDiagram
    participant S as Scraper
    participant A as Active Filings Page
    participant M as Main Search
    participant D as Details Page
    participant G as Google Drive
    
    S-&gt;&gt;A: GET /activeFilings.aspx
    A--&gt;&gt;S: HTML with table
    S-&gt;&gt;S: Extract franchise names
    
    loop For each franchise
        S-&gt;&gt;M: POST /MainSearch.aspx
        M--&gt;&gt;S: Search results
        S-&gt;&gt;D: GET /Details.aspx?id=X
        D--&gt;&gt;S: Filing details + download link
        S-&gt;&gt;D: GET /Download?file=Y
        D--&gt;&gt;S: PDF binary
        S-&gt;&gt;G: Upload PDF
    end
```

### 1.2 Data Captured at Acquisition

| Field | Type | Source | Example |
|-------|------|--------|---------|
| source_name | string | Hardcoded | &quot;MN&quot;, &quot;WI&quot; |
| filing_number | string | Portal HTML | &quot;FDD-2024-001&quot; |
| filing_date | date | Portal HTML | &quot;2024-01-15&quot; |
| franchise_name | string | Portal HTML | &quot;Subway Restaurants&quot; |
| document_url | string | Portal HTML | &quot;https://...&quot; |
| download_timestamp | timestamp | System | &quot;2024-01-20T10:30:00Z&quot; |
| file_size_bytes | integer | HTTP headers | 2456789 |
| sha256_hash | string | Computed | &quot;a1b2c3...&quot; |

## Stage 2: Document Registration

### 2.1 Deduplication Flow

```mermaid
graph TD
    A[New Document] --&gt; B{Hash Exists?}
    B --&gt;|Yes| C[Mark as Duplicate]
    B --&gt;|No| D[Extract First 10 Pages]
    D --&gt; E[Parse Franchise Info]
    E --&gt; F[Generate Name Embedding]
    F --&gt; G{Similarity &gt; 0.94?}
    G --&gt;|Yes| H{Same Issue Date?}
    G --&gt;|No| I[Create New Franchise]
    H --&gt;|Yes| J[Mark as Duplicate]
    H --&gt;|No| K{Is Amendment?}
    K --&gt;|Yes| L[Supersede Previous]
    K --&gt;|No| M[Register as New]
```

### 2.2 Franchise Matching Algorithm

```python
def match_franchise(extracted_info: dict) -&gt; Optional[UUID]:
    # 1. Exact name match
    exact_match = db.query(
        &quot;SELECT id FROM franchisors WHERE canonical_name = ?&quot;,
        extracted_info[&quot;franchise_name&quot;]
    )
    if exact_match:
        return exact_match.id
    
    # 2. Embedding similarity
    embedding = generate_embedding(extracted_info[&quot;franchise_name&quot;])
    similar = db.query(&quot;&quot;&quot;
        SELECT id, canonical_name, 
               1 - (name_embedding &lt;=&gt; ?) as similarity
        FROM franchisors
        WHERE 1 - (name_embedding &lt;=&gt; ?) &gt; 0.85
        ORDER BY similarity DESC
        LIMIT 5
    &quot;&quot;&quot;, embedding, embedding)
    
    # 3. Apply business rules
    for candidate in similar:
        if similarity &gt; 0.94:
            return candidate.id
        elif similarity &gt; 0.85:
            # Flag for human review
            mark_for_review(extracted_info, candidate)
    
    # 4. Create new franchise
    return create_franchise(extracted_info)
```

## Stage 3: Document Segmentation

### 3.1 MinerU Processing Pipeline

```mermaid
stateDiagram-v2
    [*] --&gt; DownloadFromDrive
    DownloadFromDrive --&gt; CallMinerUAPI
    CallMinerUAPI --&gt; PollStatus
    PollStatus --&gt; CheckComplete: Status?
    CheckComplete --&gt; PollStatus: Processing
    CheckComplete --&gt; DownloadJSON: Complete
    DownloadJSON --&gt; ParseLayout
    ParseLayout --&gt; IdentifySections
    IdentifySections --&gt; ValidateSections
    ValidateSections --&gt; GenerateSplits: Valid
    ValidateSections --&gt; ManualReview: Invalid
    GenerateSplits --&gt; [*]
```

### 3.2 Section Detection Logic

```python
def detect_sections(mineru_json: dict) -&gt; List[Section]:
    sections = []
    
    # Pass 1: High confidence - Title blocks
    for block in mineru_json[&quot;blocks&quot;]:
        if block[&quot;type&quot;] == &quot;title&quot;:
            match = re.search(r&quot;ITEM\s+(\d+)&quot;, block[&quot;text&quot;], re.I)
            if match:
                item_no = int(match.group(1))
                sections.append(Section(
                    item_no=item_no,
                    start_page=block[&quot;page_no&quot;],
                    confidence=0.95
                ))
    
    # Pass 2: Medium confidence - Text search
    if len(sections) &lt; 20:  # Missing sections
        for block in mineru_json[&quot;blocks&quot;]:
            if block[&quot;type&quot;] == &quot;text&quot;:
                # Complex regex patterns for each item
                patterns = get_item_patterns()
                for item_no, pattern in patterns.items():
                    if pattern.search(block[&quot;text&quot;]):
                        sections.append(Section(
                            item_no=item_no,
                            start_page=block[&quot;page_no&quot;],
                            confidence=0.75
                        ))
    
    # Pass 3: Validation and interpolation
    sections = validate_section_order(sections)
    sections = interpolate_missing(sections)
    
    return sections
```

### 3.3 PDF Splitting Process

```python
async def split_pdf(fdd_id: UUID, sections: List[Section]):
    # Download original PDF
    original_pdf = await download_from_drive(fdd.drive_file_id)
    
    # Create section PDFs
    for i, section in enumerate(sections):
        # Determine page range
        start = section.start_page
        end = sections[i+1].start_page - 1 if i+1 &lt; len(sections) else total_pages
        
        # Extract pages
        section_pdf = extract_pages(original_pdf, start, end)
        
        # Upload to Drive
        section_path = f&quot;/processed/{fdd.franchise_id}/{fdd.year}/section_{section.item_no:02d}.pdf&quot;
        file_id = await upload_to_drive(section_pdf, section_path)
        
        # Create database record
        await db.insert(&quot;fdd_sections&quot;, {
            &quot;fdd_id&quot;: fdd_id,
            &quot;item_no&quot;: section.item_no,
            &quot;start_page&quot;: start,
            &quot;end_page&quot;: end,
            &quot;drive_file_id&quot;: file_id,
            &quot;extraction_status&quot;: &quot;pending&quot;
        })
```

## Stage 4: Data Extraction

### 4.1 LLM Extraction Flow

```mermaid
graph TD
    A[Section PDF] --&gt; B[Model Selection]
    B --&gt; C{Section Type}
    C --&gt;|Simple Table| D[Ollama Local]
    C --&gt;|Complex Text| E[Gemini Pro]
    C --&gt;|Failed Once| F[OpenAI GPT-4]
    
    D --&gt; G[Load Prompt Template]
    E --&gt; G
    F --&gt; G
    
    G --&gt; H[Inject Variables]
    H --&gt; I[Call Instructor]
    I --&gt; J{Valid Schema?}
    J --&gt;|Yes| K[Business Validation]
    J --&gt;|No| L{Retry &lt; 3?}
    L --&gt;|Yes| M[Upgrade Model]
    L --&gt;|No| N[Mark Failed]
    M --&gt; I
    
    K --&gt; O{Rules Pass?}
    O --&gt;|Yes| P[Store Data]
    O --&gt;|No| Q[Flag Review]
```

### 4.2 Model Selection Matrix

| Section | Primary Model | Fallback | Criteria |
|---------|--------------|----------|----------|
| Item 5 (Fees) | Ollama phi3 | Gemini Pro | Simple table, low variance |
| Item 6 (Other Fees) | Ollama llama3:8b | Gemini Pro | Semi-structured list |
| Item 7 (Investment) | Ollama llama3:8b | Gemini Pro | Standard table format |
| Item 19 (FPR) | Gemini Pro 2.5 | GPT-4 | High variance, complex |
| Item 20 (Outlets) | Ollama llama3:8b | Gemini Pro | Structured tables |
| Item 21 (Financials) | Gemini Pro 2.5 | GPT-4 | Financial statements |
| Others | Gemini Pro 2.5 | GPT-4 | Unstructured text |

### 4.3 Prompt Engineering

```yaml
# Example: prompts/item20.yaml
system: |
  You are an expert at extracting franchise outlet data from FDD Item 20.
  Extract ALL tables showing outlet counts by year and state.
  Follow the schema exactly.

user: |
  Extract outlet information from this FDD Item 20 section:
  
  Document: {{ franchise_name }} - {{ issue_year }}
  
  Content:
  {{ section_text }}

few_shot_examples:
  - input: |
      Table No. 1
      Systemwide Outlet Summary
      For Years 2021 to 2023
      
      Outlet Type | Year | Start | Opened | Closed | End
      Franchised | 2021 | 100 | 25 | 10 | 115
      Company | 2021 | 20 | 5 | 2 | 23
    
    output:
      outlet_summary:
        - fiscal_year: 2021
          outlet_type: &quot;Franchised&quot;
          count_start: 100
          opened: 25
          closed: 10
          count_end: 115
```

### 4.4 Structured Output Enforcement

```python
from instructor import patch
from pydantic import BaseModel, validator

class Item20Summary(BaseModel):
    fiscal_year: int
    outlet_type: Literal[&quot;Franchised&quot;, &quot;Company-Owned&quot;]
    count_start: int
    opened: int
    closed: int
    transferred_in: int = 0
    transferred_out: int = 0
    count_end: int
    
    @validator(&apos;count_end&apos;)
    def validate_math(cls, v, values):
        expected = (values[&apos;count_start&apos;] + values[&apos;opened&apos;] 
                   - values[&apos;closed&apos;] + values.get(&apos;transferred_in&apos;, 0)
                   - values.get(&apos;transferred_out&apos;, 0))
        if v != expected:
            raise ValueError(f&quot;Math doesn&apos;t add up: {expected} != {v}&quot;)
        return v

# Use with Instructor
client = patch(OpenAI())
response = client.chat.completions.create(
    model=&quot;gpt-4&quot;,
    messages=[...],
    response_model=List[Item20Summary]
)
```

## Stage 5: Data Storage

### 5.1 Storage Routing Logic

```mermaid
graph TD
    A[Validated Data] --&gt; B{Item Number}
    B --&gt;|5| C[item5_initial_fees]
    B --&gt;|6| D[item6_other_fees]
    B --&gt;|7| E[item7_initial_investment]
    B --&gt;|19| F[item19_fpr]
    B --&gt;|20| G[item20_outlet_summary]
    B --&gt;|21| H[item21_financials]
    B --&gt;|Others| I[fdd_item_json]
    
    C --&gt; J[Upsert Operation]
    D --&gt; J
    E --&gt; J
    F --&gt; J
    G --&gt; J
    H --&gt; J
    I --&gt; K[JSON Insert]
    
    J --&gt; L[Update Indexes]
    K --&gt; L
    L --&gt; M[Refresh Views]
    M --&gt; N[Emit Events]
```

### 5.2 Database Transaction Flow

```python
async def store_extraction_result(section_id: UUID, item_no: int, data: dict):
    async with db.transaction() as tx:
        try:
            # 1. Route to appropriate table
            if item_no == 20:
                # Insert outlet summaries
                for record in data[&quot;outlet_summary&quot;]:
                    await tx.execute(&quot;&quot;&quot;
                        INSERT INTO item20_outlet_summary 
                        (section_id, fiscal_year, outlet_type, ...)
                        VALUES ($1, $2, $3, ...)
                        ON CONFLICT (section_id, fiscal_year, outlet_type)
                        DO UPDATE SET ...
                    &quot;&quot;&quot;, section_id, record[&quot;fiscal_year&quot;], ...)
                
                # Insert state counts
                for state in data.get(&quot;state_counts&quot;, []):
                    await tx.execute(...)
            
            elif item_no in [5, 6, 7, 19, 21]:
                # Handle other structured tables
                ...
            
            else:
                # Generic JSON storage
                await tx.execute(&quot;&quot;&quot;
                    INSERT INTO fdd_item_json (section_id, item_no, data)
                    VALUES ($1, $2, $3)
                &quot;&quot;&quot;, section_id, item_no, json.dumps(data))
            
            # 2. Update section status
            await tx.execute(&quot;&quot;&quot;
                UPDATE fdd_sections 
                SET extraction_status = &apos;success&apos;,
                    extracted_at = NOW(),
                    extraction_model = $2
                WHERE id = $1
            &quot;&quot;&quot;, section_id, model_used)
            
            # 3. Check if all sections complete
            await check_fdd_completion(tx, section.fdd_id)
            
            await tx.commit()
            
        except Exception as e:
            await tx.rollback()
            await log_extraction_error(section_id, str(e))
            raise
```

## Data Quality Checkpoints

### Quality Gates Throughout Pipeline

```mermaid
graph LR
    A[Acquisition] --&gt; A1{Valid PDF?}
    A1 --&gt; B[Registration]
    B --&gt; B1{Duplicate?}
    B1 --&gt; C[Segmentation]
    C --&gt; C1{All Sections Found?}
    C1 --&gt; D[Extraction]
    D --&gt; D1{Schema Valid?}
    D1 --&gt; D2{Business Rules?}
    D2 --&gt; E[Storage]
    E --&gt; E1{Transaction Success?}
    
    style A1 fill:#f9f,stroke:#333
    style B1 fill:#f9f,stroke:#333
    style C1 fill:#f9f,stroke:#333
    style D1 fill:#f9f,stroke:#333
    style D2 fill:#f9f,stroke:#333
    style E1 fill:#f9f,stroke:#333
```

### Error Handling Strategy

| Stage | Error Type | Handling | Recovery |
|-------|------------|----------|----------|
| Acquisition | Network timeout | Retry 3x with backoff | Re-run scraper |
| Registration | Duplicate found | Skip processing | Manual review |
| Segmentation | Missing sections | Flag for review | Manual split |
| Extraction | Schema invalid | Retry with better model | Human correction |
| Storage | Transaction fail | Rollback + retry | Investigate logs |

## Performance Characteristics

### Data Volume Metrics

| Metric | Value | Notes |
|--------|-------|-------|
| Avg PDF size | 2.5 MB | Range: 500KB - 10MB |
| Sections per FDD | 25 | Intro (0) + Items 1-23 + Appendix (24) |
| Extraction time/section | 15-30s | Depends on model |
| Total processing time | 5-10 min | Full pipeline |
| Daily throughput | 100+ FDDs | With current resources |

### Bottleneck Analysis

1. **MinerU API**: Rate limited to 10 concurrent
   - Solution: Queue management
   
2. **LLM API Calls**: Cost and rate limits
   - Solution: Local models for simple tasks
   
3. **Database Writes**: Connection pooling
   - Solution: Batch inserts where possible

## Data Lineage

Every piece of data can be traced back through:

```
Final Data → fdd_sections → fdds → scrape_metadata → Source Portal
           ↘ extraction_logs ↗
```

This enables:
- Debugging extraction issues
- Re-processing from any stage
- Audit trail for compliance
- Quality improvement analysis

---

For implementation details, see:
- [Technology Decisions](technology_decisions.md)
- [Validation Rules](../02_data_model/validation_rules.md)</file><file path="docs/database_schema.md"># Database Schema Documentation

## Overview

The FDD Pipeline uses PostgreSQL (via Supabase) as its primary database. The schema is designed to support efficient document tracking, structured data extraction, and analytics while maintaining referential integrity and audit trails.

## Entity Relationship Diagram

```mermaid
erDiagram
    franchisors ||--o{ fdds : owns
    fdds ||--o{ fdd_sections : contains
    fdds ||--o{ scrape_metadata : tracked_by
    
    fdd_sections ||--o| item5_initial_fees : has_fees
    fdd_sections ||--o{ item6_other_fees : has_fees
    fdd_sections ||--o{ item7_initial_investment : has_costs
    fdd_sections ||--o| item19_fpr : has_performance
    fdd_sections ||--o{ item20_outlet_summary : has_outlets
    fdd_sections ||--o{ item20_state_counts : has_states
    fdd_sections ||--o| item21_financials : has_financials
    fdd_sections ||--o| fdd_item_json : has_data
    
    prefect_runs ||--o{ pipeline_logs : generates
    
    franchisors {
        uuid id PK
        string canonical_name UK
        string parent_company
        string website
        string phone
        jsonb dba_names
        vector name_embedding
        timestamptz created_at
        timestamptz updated_at
    }
    
    fdds {
        uuid id PK
        uuid franchise_id FK
        date issue_date
        date amendment_date
        string document_type
        string filing_state
        string drive_path UK
        string drive_file_id UK
        integer total_pages
        uuid superseded_by_id FK
        boolean needs_review
        string processing_status
        timestamptz created_at
    }
```

## Core Tables

### franchisors
Stores canonical franchise information with deduplication support.

```sql
CREATE TABLE franchisors (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    canonical_name TEXT NOT NULL UNIQUE,
    parent_company TEXT,
    website TEXT,
    phone TEXT,
    email TEXT,
    address JSONB, -- {street, city, state, zip}
    dba_names JSONB DEFAULT &apos;[]&apos;::jsonb, -- array of alternate names
    name_embedding vector(384), -- for similarity search
    created_at TIMESTAMPTZ DEFAULT now(),
    updated_at TIMESTAMPTZ DEFAULT now()
);

-- Indexes
CREATE INDEX idx_franchisors_canonical_name ON franchisors(canonical_name);
CREATE INDEX idx_franchisors_parent_company ON franchisors(parent_company);
CREATE INDEX idx_franchisors_embedding ON franchisors USING ivfflat (name_embedding vector_cosine_ops);
```

### fdds
Tracks individual FDD documents with versioning and supersession.

```sql
CREATE TABLE fdds (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    franchise_id UUID REFERENCES franchisors(id) ON DELETE CASCADE,
    issue_date DATE NOT NULL,
    amendment_date DATE,
    is_amendment BOOLEAN GENERATED ALWAYS AS (amendment_date IS NOT NULL) STORED,
    document_type TEXT NOT NULL CHECK (document_type IN (&apos;Initial&apos;, &apos;Amendment&apos;, &apos;Renewal&apos;)),
    filing_state TEXT NOT NULL,
    filing_number TEXT,
    drive_path TEXT NOT NULL UNIQUE,
    drive_file_id TEXT NOT NULL UNIQUE,
    sha256_hash CHAR(64),
    total_pages INTEGER,
    language_code TEXT DEFAULT &apos;en&apos;,
    superseded_by_id UUID REFERENCES fdds(id),
    duplicate_of_id UUID REFERENCES fdds(id),
    needs_review BOOLEAN DEFAULT false,
    processing_status TEXT DEFAULT &apos;pending&apos; CHECK (processing_status IN (&apos;pending&apos;, &apos;processing&apos;, &apos;completed&apos;, &apos;failed&apos;)),
    created_at TIMESTAMPTZ DEFAULT now(),
    processed_at TIMESTAMPTZ
);

-- Indexes
CREATE INDEX idx_fdds_franchise_id ON fdds(franchise_id);
CREATE INDEX idx_fdds_issue_date ON fdds(issue_date);
CREATE INDEX idx_fdds_filing_state ON fdds(filing_state);
CREATE INDEX idx_fdds_processing_status ON fdds(processing_status);
CREATE INDEX idx_fdds_needs_review ON fdds(needs_review) WHERE needs_review = true;
```

### scrape_metadata
Captures web scraping provenance and metadata.

```sql
CREATE TABLE scrape_metadata (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    fdd_id UUID REFERENCES fdds(id) ON DELETE CASCADE,
    source_name TEXT NOT NULL, -- &apos;MN&apos;, &apos;WI&apos;
    source_url TEXT NOT NULL,
    filing_metadata JSONB, -- portal-specific fields
    prefect_run_id UUID,
    scraped_at TIMESTAMPTZ DEFAULT now()
);

-- Indexes
CREATE INDEX idx_scrape_metadata_fdd_id ON scrape_metadata(fdd_id);
CREATE INDEX idx_scrape_metadata_source ON scrape_metadata(source_name, scraped_at);
```

### fdd_sections
Maps document sections after MinerU segmentation.

```sql
CREATE TABLE fdd_sections (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    fdd_id UUID REFERENCES fdds(id) ON DELETE CASCADE,
    item_no INTEGER NOT NULL CHECK (item_no &gt;= 0 AND item_no &lt;= 24), -- 0=intro, 24=appendix
    item_name TEXT,
    start_page INTEGER NOT NULL,
    end_page INTEGER NOT NULL CHECK (end_page &gt;= start_page),
    drive_path TEXT,
    drive_file_id TEXT,
    extraction_status TEXT DEFAULT &apos;pending&apos; CHECK (extraction_status IN (&apos;pending&apos;, &apos;processing&apos;, &apos;success&apos;, &apos;failed&apos;, &apos;skipped&apos;)),
    extraction_model TEXT, -- which LLM was used
    extraction_attempts INTEGER DEFAULT 0,
    needs_review BOOLEAN DEFAULT false,
    created_at TIMESTAMPTZ DEFAULT now(),
    extracted_at TIMESTAMPTZ,
    
    CONSTRAINT unique_fdd_section UNIQUE (fdd_id, item_no)
);

-- Indexes
CREATE INDEX idx_fdd_sections_fdd_id ON fdd_sections(fdd_id);
CREATE INDEX idx_fdd_sections_status ON fdd_sections(extraction_status);
CREATE INDEX idx_fdd_sections_needs_review ON fdd_sections(needs_review) WHERE needs_review = true;
```

## Structured Data Tables

### item5_initial_fees
Initial franchise fees and conditions.

```sql
CREATE TABLE item5_initial_fees (
    section_id UUID REFERENCES fdd_sections(id) ON DELETE CASCADE,
    fee_name TEXT NOT NULL,
    amount_cents BIGINT NOT NULL CHECK (amount_cents &gt;= 0),
    refundable BOOLEAN DEFAULT false,
    refund_conditions TEXT,
    due_at TEXT, -- &apos;Signing&apos;, &apos;Training&apos;, &apos;Opening&apos;
    notes TEXT,
    
    PRIMARY KEY (section_id, fee_name)
);

-- Common fee_name values:
-- &apos;Initial Franchise Fee&apos;, &apos;Territory Fee&apos;, &apos;Training Fee&apos;
```

### item6_other_fees
Ongoing and other fees.

```sql
CREATE TABLE item6_other_fees (
    section_id UUID REFERENCES fdd_sections(id) ON DELETE CASCADE,
    fee_name TEXT NOT NULL,
    amount_cents BIGINT CHECK (amount_cents &gt;= 0),
    amount_percentage NUMERIC(5,2) CHECK (amount_percentage &gt;= 0 AND amount_percentage &lt;= 100),
    frequency TEXT NOT NULL, -- &apos;Weekly&apos;, &apos;Monthly&apos;, &apos;Annual&apos;, &apos;One-time&apos;
    calculation_basis TEXT, -- &apos;Gross Sales&apos;, &apos;Net Sales&apos;, &apos;Fixed&apos;
    minimum_cents BIGINT CHECK (minimum_cents &gt;= 0),
    maximum_cents BIGINT CHECK (maximum_cents &gt;= minimum_cents),
    remarks TEXT,
    
    PRIMARY KEY (section_id, fee_name),
    CONSTRAINT check_amount_or_percentage CHECK (
        (amount_cents IS NOT NULL AND amount_percentage IS NULL) OR
        (amount_cents IS NULL AND amount_percentage IS NOT NULL)
    )
);

-- Common fee_name values:
-- &apos;Royalty Fee&apos;, &apos;Marketing Fee&apos;, &apos;Technology Fee&apos;, &apos;Supply Chain Fee&apos;
```

### item7_initial_investment
Estimated initial investment breakdown.

```sql
CREATE TABLE item7_initial_investment (
    section_id UUID REFERENCES fdd_sections(id) ON DELETE CASCADE,
    category TEXT NOT NULL,
    low_cents BIGINT CHECK (low_cents &gt;= 0),
    high_cents BIGINT CHECK (high_cents &gt;= low_cents),
    when_due TEXT,
    to_whom TEXT,
    remarks TEXT,
    
    PRIMARY KEY (section_id, category)
);

-- Common categories:
-- &apos;Real Estate&apos;, &apos;Equipment&apos;, &apos;Inventory&apos;, &apos;Working Capital&apos;
```

### item19_fpr
Financial Performance Representations.

```sql
CREATE TABLE item19_fpr (
    section_id UUID PRIMARY KEY REFERENCES fdd_sections(id) ON DELETE CASCADE,
    disclosure_type TEXT, -- &apos;Historical&apos;, &apos;Projected&apos;, &apos;None&apos;
    methodology TEXT, -- narrative description
    sample_size INTEGER CHECK (sample_size &gt; 0),
    sample_description TEXT,
    time_period TEXT,
    
    -- Common financial metrics (all nullable)
    average_revenue_cents BIGINT CHECK (average_revenue_cents &gt;= 0),
    median_revenue_cents BIGINT CHECK (median_revenue_cents &gt;= 0),
    low_revenue_cents BIGINT CHECK (low_revenue_cents &gt;= 0),
    high_revenue_cents BIGINT CHECK (high_revenue_cents &gt;= 0),
    
    average_profit_cents BIGINT,
    median_profit_cents BIGINT,
    profit_margin_percentage NUMERIC(5,2),
    
    -- For complex/varied data
    additional_metrics JSONB DEFAULT &apos;{}&apos;::jsonb,
    tables_data JSONB DEFAULT &apos;[]&apos;::jsonb, -- array of table representations
    
    disclaimers TEXT,
    created_at TIMESTAMPTZ DEFAULT now()
);
```

### item20_outlet_summary
System-wide outlet information by year.

```sql
CREATE TABLE item20_outlet_summary (
    section_id UUID REFERENCES fdd_sections(id) ON DELETE CASCADE,
    fiscal_year SMALLINT NOT NULL CHECK (fiscal_year &gt;= 1900 AND fiscal_year &lt;= 2100),
    outlet_type TEXT NOT NULL CHECK (outlet_type IN (&apos;Franchised&apos;, &apos;Company-Owned&apos;)),
    
    -- Annual movements
    count_start INTEGER NOT NULL CHECK (count_start &gt;= 0),
    opened INTEGER NOT NULL DEFAULT 0 CHECK (opened &gt;= 0),
    closed INTEGER NOT NULL DEFAULT 0 CHECK (closed &gt;= 0),
    transferred_in INTEGER NOT NULL DEFAULT 0 CHECK (transferred_in &gt;= 0),
    transferred_out INTEGER NOT NULL DEFAULT 0 CHECK (transferred_out &gt;= 0),
    count_end INTEGER NOT NULL CHECK (count_end &gt;= 0),
    
    -- Calculated check
    CONSTRAINT outlet_math_check CHECK (
        count_end = count_start + opened - closed + transferred_in - transferred_out
    ),
    
    PRIMARY KEY (section_id, fiscal_year, outlet_type)
);

-- State-by-state breakdown
CREATE TABLE item20_state_counts (
    section_id UUID REFERENCES fdd_sections(id) ON DELETE CASCADE,
    state_code CHAR(2) NOT NULL,
    franchised_count INTEGER DEFAULT 0 CHECK (franchised_count &gt;= 0),
    company_owned_count INTEGER DEFAULT 0 CHECK (company_owned_count &gt;= 0),
    
    PRIMARY KEY (section_id, state_code)
);
```

### item21_financials
Summarized financial statements.

```sql
CREATE TABLE item21_financials (
    section_id UUID PRIMARY KEY REFERENCES fdd_sections(id) ON DELETE CASCADE,
    fiscal_year SMALLINT CHECK (fiscal_year &gt;= 1900 AND fiscal_year &lt;= 2100),
    fiscal_year_end DATE,
    
    -- Income Statement
    total_revenue_cents BIGINT CHECK (total_revenue_cents &gt;= 0),
    franchise_revenue_cents BIGINT CHECK (franchise_revenue_cents &gt;= 0),
    cost_of_goods_cents BIGINT CHECK (cost_of_goods_cents &gt;= 0),
    gross_profit_cents BIGINT,
    operating_expenses_cents BIGINT CHECK (operating_expenses_cents &gt;= 0),
    operating_income_cents BIGINT,
    net_income_cents BIGINT,
    
    -- Balance Sheet
    total_assets_cents BIGINT CHECK (total_assets_cents &gt;= 0),
    current_assets_cents BIGINT CHECK (current_assets_cents &gt;= 0),
    total_liabilities_cents BIGINT CHECK (total_liabilities_cents &gt;= 0),
    current_liabilities_cents BIGINT CHECK (current_liabilities_cents &gt;= 0),
    total_equity_cents BIGINT,
    
    -- Audit info
    auditor_name TEXT,
    audit_opinion TEXT,
    
    created_at TIMESTAMPTZ DEFAULT now()
);
```

### fdd_item_json
Generic storage for all other items.

```sql
CREATE TABLE fdd_item_json (
    section_id UUID PRIMARY KEY REFERENCES fdd_sections(id) ON DELETE CASCADE,
    item_no INTEGER NOT NULL,
    data JSONB NOT NULL DEFAULT &apos;{}&apos;::jsonb,
    schema_version TEXT DEFAULT &apos;1.0&apos;,
    validated BOOLEAN DEFAULT false,
    validation_errors JSONB,
    created_at TIMESTAMPTZ DEFAULT now(),
    updated_at TIMESTAMPTZ DEFAULT now()
);

-- Index for querying specific fields within JSON
CREATE INDEX idx_fdd_item_json_data ON fdd_item_json USING gin(data);
```

## Operational Tables

### pipeline_logs
Structured logging for all pipeline operations.

```sql
CREATE TABLE pipeline_logs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    prefect_run_id UUID,
    task_name TEXT,
    level TEXT NOT NULL CHECK (level IN (&apos;DEBUG&apos;, &apos;INFO&apos;, &apos;WARNING&apos;, &apos;ERROR&apos;, &apos;CRITICAL&apos;)),
    message TEXT NOT NULL,
    context JSONB DEFAULT &apos;{}&apos;::jsonb,
    created_at TIMESTAMPTZ DEFAULT now()
);

-- Indexes
CREATE INDEX idx_pipeline_logs_run_id ON pipeline_logs(prefect_run_id);
CREATE INDEX idx_pipeline_logs_level ON pipeline_logs(level, created_at DESC);
CREATE INDEX idx_pipeline_logs_created ON pipeline_logs(created_at DESC);

-- Partition by month for performance
CREATE TABLE pipeline_logs_2024_01 PARTITION OF pipeline_logs
    FOR VALUES FROM (&apos;2024-01-01&apos;) TO (&apos;2024-02-01&apos;);
```

### prefect_runs
Track Prefect flow runs for debugging.

```sql
CREATE TABLE prefect_runs (
    id UUID PRIMARY KEY,
    flow_name TEXT NOT NULL,
    deployment_name TEXT,
    state TEXT NOT NULL,
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    parameters JSONB,
    context JSONB,
    created_at TIMESTAMPTZ DEFAULT now()
);
```

## Views

### v_latest_fdds
Shows only the most recent FDD per franchise.

```sql
CREATE VIEW v_latest_fdds AS
SELECT DISTINCT ON (f.id) 
    f.id as franchise_id,
    f.canonical_name,
    fdd.*
FROM franchisors f
JOIN fdds fdd ON f.id = fdd.franchise_id
WHERE fdd.superseded_by_id IS NULL
    AND fdd.duplicate_of_id IS NULL
ORDER BY f.id, fdd.issue_date DESC, fdd.amendment_date DESC NULLS LAST;
```

### v_extraction_status
Summary of extraction progress.

```sql
CREATE VIEW v_extraction_status AS
SELECT 
    fdd.id as fdd_id,
    fdd.franchise_id,
    f.canonical_name,
    COUNT(*) as total_sections,
    COUNT(*) FILTER (WHERE fs.extraction_status = &apos;success&apos;) as extracted_sections,
    COUNT(*) FILTER (WHERE fs.extraction_status = &apos;failed&apos;) as failed_sections,
    COUNT(*) FILTER (WHERE fs.needs_review) as needs_review,
    ROUND(100.0 * COUNT(*) FILTER (WHERE fs.extraction_status = &apos;success&apos;) / COUNT(*), 2) as success_rate
FROM fdds fdd
JOIN franchisors f ON fdd.franchise_id = f.id
JOIN fdd_sections fs ON fdd.id = fs.fdd_id
GROUP BY fdd.id, fdd.franchise_id, f.canonical_name;
```

## Row Level Security (RLS)

```sql
-- Enable RLS on all tables
ALTER TABLE franchisors ENABLE ROW LEVEL SECURITY;
ALTER TABLE fdds ENABLE ROW LEVEL SECURITY;
-- ... etc for all tables

-- Public read access
CREATE POLICY &quot;Public read access&quot; ON franchisors
    FOR SELECT USING (true);

-- Service role has full access
CREATE POLICY &quot;Service role full access&quot; ON franchisors
    FOR ALL USING (auth.jwt() -&gt;&gt; &apos;role&apos; = &apos;service_role&apos;);

-- Anon users can only read completed data
CREATE POLICY &quot;Anon read completed&quot; ON fdds
    FOR SELECT USING (
        processing_status = &apos;completed&apos; 
        AND auth.jwt() -&gt;&gt; &apos;role&apos; = &apos;anon&apos;
    );
```

## Indexes Strategy

### Performance Indexes
- Foreign keys: All FK columns are indexed
- Common queries: Composite indexes for frequent filter combinations
- JSON fields: GIN indexes for JSONB columns
- Text search: GiST indexes for full-text search (future)

### Example Composite Indexes
```sql
-- For finding documents to process
CREATE INDEX idx_fdds_to_process ON fdds(processing_status, created_at) 
    WHERE processing_status = &apos;pending&apos;;

-- For extraction workload
CREATE INDEX idx_sections_to_extract ON fdd_sections(extraction_status, item_no) 
    WHERE extraction_status IN (&apos;pending&apos;, &apos;failed&apos;);
```

## Maintenance

### Vacuum Strategy
```sql
-- Aggressive autovacuum for high-write tables
ALTER TABLE pipeline_logs SET (
    autovacuum_vacuum_scale_factor = 0.1,
    autovacuum_analyze_scale_factor = 0.05
);
```

### Archival Strategy
- `pipeline_logs` older than 90 days → compressed archive
- `scrape_metadata` older than 1 year → cold storage
- Failed extraction attempts → retain for analysis

## Migration Management

All schema changes are managed through numbered migration files:

```
migrations/
├── 001_initial_schema.sql
├── 002_add_franchisors_embedding.sql
├── 003_add_item_tables.sql
├── 004_add_rls_policies.sql
└── 005_add_performance_indexes.sql
```

Run migrations with:
```bash
supabase db push
```

---

For Pydantic models that map to these tables, see [Pydantic Models Documentation](pydantic_models.md).</file><file path="docs/mineru_processing.md"># MinerU PDF Processing Documentation

## Overview

The FDD Pipeline now uses MinerU&apos;s Web API exclusively for PDF processing. This provides superior layout analysis and text extraction compared to local processing methods. All PDFs are processed through MinerU&apos;s cloud service, with results automatically stored in Google Drive.

## Architecture Changes

### Previous Architecture (Removed)
- Local MinerU installation with magic-pdf library
- Complex document_processing.py with 1000+ lines
- Multiple processing modes (API vs local)
- Manual layout analysis and section detection

### New Architecture
- Single MinerU Web API implementation
- Streamlined mineru_processing.py (~400 lines)
- Automatic Google Drive storage with UUID-based organization
- Consistent processing across all environments

## File Organization

### UUID-Based Tracking
Every FDD document is assigned a UUID at download time. This UUID is used throughout the pipeline:

1. **PDF Filename**: `{fdd_uuid}_{franchise_name}_{date}.pdf`
2. **Google Drive Folder**: `/{fdd_uuid}/`
3. **MinerU Output Files**: 
   - `/{fdd_uuid}/{franchise_name}_mineru.md`
   - `/{fdd_uuid}/{franchise_name}_layout.json`

### Google Drive Structure
```
Root Folder (1df-zMpAYkfM0EhDTeqH36RpmwKfmYApj)
├── {fdd_uuid_1}/
│   ├── {franchise_name}_mineru.md
│   └── {franchise_name}_layout.json
├── {fdd_uuid_2}/
│   ├── {franchise_name}_mineru.md
│   └── {franchise_name}_layout.json
└── ...
```

## Authentication

MinerU requires GitHub authentication. The auth state is saved locally:

1. First run: Browser opens for GitHub login
2. Auth saved to: `{project_root}/mineru_auth.json`
3. Subsequent runs: Uses saved authentication

## API Integration

### Key Functions

#### `process_document_with_mineru()`
Main processing function that:
1. Submits PDF to MinerU API
2. Polls for completion
3. Downloads results (markdown and JSON)
4. Stores results in Google Drive
5. Returns processing metadata

#### `MinerUProcessor` Class
- Handles authentication
- Manages API requests
- Integrates with Google Drive
- Provides retry logic

### Usage Example

```python
from tasks.mineru_processing import process_document_with_mineru
from uuid import uuid4

# Process a PDF
results = await process_document_with_mineru(
    pdf_url=&quot;https://example.com/fdd.pdf&quot;,
    fdd_id=uuid4(),
    franchise_name=&quot;Example Franchise&quot;,
    timeout_seconds=300
)

# Results include:
# - task_id: MinerU processing task ID
# - mineru_results: URLs for markdown and JSON
# - drive_files: Google Drive file IDs and paths
# - fdd_uuid: The UUID used for tracking
# - processed_at: Timestamp
```

## Configuration

Add to your environment:
```bash
# Google Drive folder for MinerU outputs
GDRIVE_FOLDER_ID=1df-zMpAYkfM0EhDTeqH36RpmwKfmYApj

# MinerU auth file location (auto-created)
MINERU_AUTH_FILE=mineru_auth.json
```

## Migration Notes

### Removed Components
- `tasks/document_processing.py` - Replaced by `mineru_processing.py`
- `src/MinerU/` directory - Cleaned up experimental code
- Local MinerU configurations - No longer needed

### Updated Components
- `flows/process_single_pdf.py` - Uses new MinerU processor
- Import statements - Now use `models.document_models`
- Section detection - Simplified placeholder implementation

### Backward Compatibility
For components that depend on old classes:
- `DocumentLayout`, `SectionBoundary`, `LayoutElement` → Moved to `models.document_models.py`
- `process_document_layout` → Replaced by `process_document_with_mineru`

## Benefits

1. **Simplicity**: Single implementation path
2. **Reliability**: Cloud-based processing with retry logic
3. **Traceability**: UUID-based file tracking
4. **Storage**: Automatic Google Drive organization
5. **Maintenance**: 70% less code to maintain

## Troubleshooting

### Authentication Issues
- Delete `mineru_auth.json` and re-authenticate
- Ensure GitHub account has access to MinerU

### Processing Failures
- Check MinerU task status in logs
- Verify PDF URL is accessible
- Review Google Drive permissions

### Storage Issues
- Confirm Google Drive service account permissions
- Check folder ID configuration
- Verify available storage quota</file><file path="docs/pydantic_models.md"># Pydantic Models Documentation

## Overview

This document defines the Pydantic models used throughout the FDD Pipeline for data validation, serialization, and type safety. These models map directly to the database schema while providing additional validation logic and computed fields.

## Core Models

### Franchisor Models

```python
from pydantic import BaseModel, Field, validator, root_validator
from typing import Optional, List, Dict, Any
from datetime import datetime
from uuid import UUID
import re

class Address(BaseModel):
    &quot;&quot;&quot;Embedded address model for franchisor addresses&quot;&quot;&quot;
    street: str
    city: str
    state: str = Field(..., regex=&quot;^[A-Z]{2}$&quot;)
    zip_code: str = Field(..., alias=&quot;zip&quot;)
    
    @validator(&apos;zip_code&apos;)
    def validate_zip(cls, v):
        if not re.match(r&quot;^\d{5}(-\d{4})?$&quot;, v):
            raise ValueError(&quot;Invalid ZIP code format&quot;)
        return v

class FranchisorBase(BaseModel):
    &quot;&quot;&quot;Base franchisor model for creation/updates&quot;&quot;&quot;
    canonical_name: str = Field(..., min_length=1, max_length=255)
    parent_company: Optional[str] = None
    website: Optional[str] = None
    phone: Optional[str] = None
    email: Optional[str] = None
    address: Optional[Address] = None
    dba_names: List[str] = Field(default_factory=list)
    
    @validator(&apos;canonical_name&apos;)
    def clean_canonical_name(cls, v):
        &quot;&quot;&quot;Normalize franchise names&quot;&quot;&quot;
        return v.strip().title()
    
    @validator(&apos;website&apos;)
    def validate_website(cls, v):
        if v and not v.startswith((&apos;http://&apos;, &apos;https://&apos;)):
            v = f&quot;https://{v}&quot;
        return v
    
    @validator(&apos;phone&apos;)
    def validate_phone(cls, v):
        if v:
            # Remove all non-digits
            digits = re.sub(r&apos;\D&apos;, &apos;&apos;, v)
            if len(digits) == 10:
                return f&quot;({digits[:3]}) {digits[3:6]}-{digits[6:]}&quot;
            elif len(digits) == 11 and digits[0] == &apos;1&apos;:
                return f&quot;+1 ({digits[1:4]}) {digits[4:7]}-{digits[7:]}&quot;
            else:
                raise ValueError(&quot;Invalid phone number format&quot;)
        return v
    
    @validator(&apos;email&apos;)
    def validate_email(cls, v):
        if v and not re.match(r&quot;[^@]+@[^@]+\.[^@]+&quot;, v):
            raise ValueError(&quot;Invalid email format&quot;)
        return v

class FranchisorCreate(FranchisorBase):
    &quot;&quot;&quot;Model for creating new franchisors&quot;&quot;&quot;
    pass

class FranchisorUpdate(BaseModel):
    &quot;&quot;&quot;Model for updating franchisors (all fields optional)&quot;&quot;&quot;
    canonical_name: Optional[str] = Field(None, min_length=1, max_length=255)
    parent_company: Optional[str] = None
    website: Optional[str] = None
    phone: Optional[str] = None
    email: Optional[str] = None
    address: Optional[Address] = None
    dba_names: Optional[List[str]] = None

class Franchisor(FranchisorBase):
    &quot;&quot;&quot;Complete franchisor model with DB fields&quot;&quot;&quot;
    id: UUID
    name_embedding: Optional[List[float]] = Field(None, description=&quot;384-dim vector&quot;)
    created_at: datetime
    updated_at: datetime
    
    class Config:
        orm_mode = True
```

### FDD Document Models

```python
from enum import Enum

class DocumentType(str, Enum):
    INITIAL = &quot;Initial&quot;
    AMENDMENT = &quot;Amendment&quot;
    RENEWAL = &quot;Renewal&quot;

class ProcessingStatus(str, Enum):
    PENDING = &quot;pending&quot;
    PROCESSING = &quot;processing&quot;
    COMPLETED = &quot;completed&quot;
    FAILED = &quot;failed&quot;

class FDDBase(BaseModel):
    &quot;&quot;&quot;Base FDD model&quot;&quot;&quot;
    franchise_id: UUID
    issue_date: date
    amendment_date: Optional[date] = None
    document_type: DocumentType
    filing_state: str = Field(..., regex=&quot;^[A-Z]{2}$&quot;)
    filing_number: Optional[str] = None
    drive_path: str
    drive_file_id: str
    sha256_hash: Optional[str] = Field(None, regex=&quot;^[a-f0-9]{64}$&quot;)
    total_pages: Optional[int] = Field(None, gt=0)
    language_code: str = Field(default=&quot;en&quot;, regex=&quot;^[a-z]{2}$&quot;)
    
    @root_validator
    def validate_amendment(cls, values):
        &quot;&quot;&quot;Ensure amendment_date is set for Amendment type&quot;&quot;&quot;
        doc_type = values.get(&apos;document_type&apos;)
        amendment_date = values.get(&apos;amendment_date&apos;)
        
        if doc_type == DocumentType.AMENDMENT and not amendment_date:
            raise ValueError(&quot;Amendment date required for Amendment documents&quot;)
        return values
    
    @validator(&apos;drive_path&apos;)
    def validate_drive_path(cls, v):
        &quot;&quot;&quot;Ensure valid Google Drive path format&quot;&quot;&quot;
        if not v.startswith(&apos;/&apos;):
            raise ValueError(&quot;Drive path must start with /&quot;)
        return v

class FDDCreate(FDDBase):
    &quot;&quot;&quot;Model for creating new FDD records&quot;&quot;&quot;
    pass

class FDD(FDDBase):
    &quot;&quot;&quot;Complete FDD model with all fields&quot;&quot;&quot;
    id: UUID
    is_amendment: bool
    superseded_by_id: Optional[UUID] = None
    duplicate_of_id: Optional[UUID] = None
    needs_review: bool = False
    processing_status: ProcessingStatus = ProcessingStatus.PENDING
    created_at: datetime
    processed_at: Optional[datetime] = None
    
    class Config:
        orm_mode = True
        use_enum_values = True
```

### Section Models

```python
class ExtractionStatus(str, Enum):
    PENDING = &quot;pending&quot;
    PROCESSING = &quot;processing&quot;
    SUCCESS = &quot;success&quot;
    FAILED = &quot;failed&quot;
    SKIPPED = &quot;skipped&quot;

class FDDSectionBase(BaseModel):
    &quot;&quot;&quot;Base model for FDD sections&quot;&quot;&quot;
    fdd_id: UUID
    item_no: int = Field(..., ge=0, le=24)
    item_name: Optional[str] = None
    start_page: int = Field(..., gt=0)
    end_page: int = Field(..., gt=0)
    drive_path: Optional[str] = None
    drive_file_id: Optional[str] = None
    
    @root_validator
    def validate_page_range(cls, values):
        start = values.get(&apos;start_page&apos;)
        end = values.get(&apos;end_page&apos;)
        if start and end and end &lt; start:
            raise ValueError(&quot;end_page must be &gt;= start_page&quot;)
        return values
    
    @validator(&apos;item_no&apos;)
    def validate_item_no(cls, v):
        &quot;&quot;&quot;Map item numbers to standard names&quot;&quot;&quot;
        item_names = {
            0: &quot;Cover/Introduction&quot;,
            1: &quot;The Franchisor and Any Parents, Predecessors, and Affiliates&quot;,
            2: &quot;Business Experience&quot;,
            3: &quot;Litigation&quot;,
            4: &quot;Bankruptcy&quot;,
            5: &quot;Initial Fees&quot;,
            6: &quot;Other Fees&quot;,
            7: &quot;Estimated Initial Investment&quot;,
            8: &quot;Restrictions on Sources of Products and Services&quot;,
            9: &quot;Financing&quot;,
            10: &quot;Franchisor&apos;s Assistance, Advertising, Computer Systems, and Training&quot;,
            11: &quot;Territory&quot;,
            12: &quot;Trademarks&quot;,
            13: &quot;Patents, Copyrights, and Proprietary Information&quot;,
            14: &quot;Obligation to Participate in the Actual Operation of the Franchise Business&quot;,
            15: &quot;Termination, Cancellation, and Renewal of the Franchise&quot;,
            16: &quot;Public Figures&quot;,
            17: &quot;Financial Performance Representations&quot;,
            18: &quot;Contacts&quot;,
            19: &quot;Financial Performance Representations&quot;,  # Note: duplicate with 17
            20: &quot;Outlets and Franchise Information&quot;,
            21: &quot;Financial Statements&quot;,
            22: &quot;Contracts&quot;,
            23: &quot;Receipts&quot;,
            24: &quot;Appendix/Exhibits&quot;
        }
        return v

class FDDSection(FDDSectionBase):
    &quot;&quot;&quot;Complete section model&quot;&quot;&quot;
    id: UUID
    extraction_status: ExtractionStatus = ExtractionStatus.PENDING
    extraction_model: Optional[str] = None
    extraction_attempts: int = 0
    needs_review: bool = False
    created_at: datetime
    extracted_at: Optional[datetime] = None
    
    class Config:
        orm_mode = True
```

## Structured Data Models

### Item 5 - Initial Fees

```python
class DueAt(str, Enum):
    SIGNING = &quot;Signing&quot;
    TRAINING = &quot;Training&quot;
    OPENING = &quot;Opening&quot;
    OTHER = &quot;Other&quot;

class InitialFeeBase(BaseModel):
    &quot;&quot;&quot;Base model for initial fees&quot;&quot;&quot;
    fee_name: str = Field(..., min_length=1)
    amount_cents: int = Field(..., ge=0)
    refundable: bool = False
    refund_conditions: Optional[str] = None
    due_at: Optional[DueAt] = None
    notes: Optional[str] = None
    
    @validator(&apos;amount_cents&apos;)
    def validate_reasonable_amount(cls, v):
        &quot;&quot;&quot;Ensure amount is reasonable (&lt; $10M)&quot;&quot;&quot;
        if v &gt; 1_000_000_000:  # $10M in cents
            raise ValueError(&quot;Amount exceeds reasonable maximum&quot;)
        return v
    
    @property
    def amount_dollars(self) -&gt; float:
        &quot;&quot;&quot;Convert cents to dollars&quot;&quot;&quot;
        return self.amount_cents / 100

class InitialFee(InitialFeeBase):
    &quot;&quot;&quot;Initial fee with section reference&quot;&quot;&quot;
    section_id: UUID
    
    class Config:
        orm_mode = True
```

### Item 6 - Other Fees

```python
class FeeFrequency(str, Enum):
    WEEKLY = &quot;Weekly&quot;
    MONTHLY = &quot;Monthly&quot;
    QUARTERLY = &quot;Quarterly&quot;
    ANNUAL = &quot;Annual&quot;
    ONE_TIME = &quot;One-time&quot;
    AS_INCURRED = &quot;As Incurred&quot;

class CalculationBasis(str, Enum):
    GROSS_SALES = &quot;Gross Sales&quot;
    NET_SALES = &quot;Net Sales&quot;
    FIXED = &quot;Fixed&quot;
    VARIABLE = &quot;Variable&quot;
    OTHER = &quot;Other&quot;

class OtherFeeBase(BaseModel):
    &quot;&quot;&quot;Base model for ongoing/other fees&quot;&quot;&quot;
    fee_name: str = Field(..., min_length=1)
    amount_cents: Optional[int] = Field(None, ge=0)
    amount_percentage: Optional[float] = Field(None, ge=0, le=100)
    frequency: FeeFrequency
    calculation_basis: Optional[CalculationBasis] = None
    minimum_cents: Optional[int] = Field(None, ge=0)
    maximum_cents: Optional[int] = Field(None, ge=0)
    remarks: Optional[str] = None
    
    @root_validator
    def validate_amount_type(cls, values):
        &quot;&quot;&quot;Ensure either amount_cents OR amount_percentage is set&quot;&quot;&quot;
        cents = values.get(&apos;amount_cents&apos;)
        pct = values.get(&apos;amount_percentage&apos;)
        
        if (cents is None and pct is None) or (cents is not None and pct is not None):
            raise ValueError(&quot;Must specify either amount_cents or amount_percentage, not both&quot;)
        return values
    
    @root_validator
    def validate_min_max(cls, values):
        &quot;&quot;&quot;Ensure max &gt;= min if both specified&quot;&quot;&quot;
        min_val = values.get(&apos;minimum_cents&apos;)
        max_val = values.get(&apos;maximum_cents&apos;)
        
        if min_val is not None and max_val is not None and max_val &lt; min_val:
            raise ValueError(&quot;maximum_cents must be &gt;= minimum_cents&quot;)
        return values
    
    @validator(&apos;amount_percentage&apos;)
    def validate_percentage(cls, v):
        &quot;&quot;&quot;Common sense check for percentages&quot;&quot;&quot;
        if v is not None and v &gt; 50:
            # Flag unusually high percentages for review
            # but don&apos;t reject - some fees can be high
            pass
        return v

class OtherFee(OtherFeeBase):
    &quot;&quot;&quot;Other fee with section reference&quot;&quot;&quot;
    section_id: UUID
    
    class Config:
        orm_mode = True
```

### Item 7 - Initial Investment

```python
class InitialInvestmentBase(BaseModel):
    &quot;&quot;&quot;Base model for initial investment items&quot;&quot;&quot;
    category: str = Field(..., min_length=1)
    low_cents: Optional[int] = Field(None, ge=0)
    high_cents: Optional[int] = Field(None, ge=0)
    when_due: Optional[str] = None
    to_whom: Optional[str] = None
    remarks: Optional[str] = None
    
    @root_validator
    def validate_range(cls, values):
        &quot;&quot;&quot;Ensure high &gt;= low and at least one is set&quot;&quot;&quot;
        low = values.get(&apos;low_cents&apos;)
        high = values.get(&apos;high_cents&apos;)
        
        if low is None and high is None:
            raise ValueError(&quot;At least one of low_cents or high_cents must be set&quot;)
        
        if low is not None and high is not None and high &lt; low:
            raise ValueError(&quot;high_cents must be &gt;= low_cents&quot;)
        
        return values
    
    @validator(&apos;category&apos;)
    def standardize_category(cls, v):
        &quot;&quot;&quot;Standardize common category names&quot;&quot;&quot;
        category_map = {
            &apos;REAL ESTATE&apos;: &apos;Real Estate&apos;,
            &apos;EQUIPMENT&apos;: &apos;Equipment&apos;,
            &apos;INVENTORY&apos;: &apos;Inventory&apos;,
            &apos;WORKING CAPITAL&apos;: &apos;Working Capital&apos;,
            &apos;TRAINING&apos;: &apos;Training&apos;,
            &apos;FRANCHISE FEE&apos;: &apos;Initial Franchise Fee&apos;
        }
        return category_map.get(v.upper(), v)

class InitialInvestment(InitialInvestmentBase):
    &quot;&quot;&quot;Initial investment with section reference&quot;&quot;&quot;
    section_id: UUID
    
    class Config:
        orm_mode = True

class InitialInvestmentSummary(BaseModel):
    &quot;&quot;&quot;Computed summary of initial investment&quot;&quot;&quot;
    section_id: UUID
    total_items: int
    total_low_cents: int
    total_high_cents: int
    items: List[InitialInvestment]
    
    @validator(&apos;total_low_cents&apos;, &apos;total_high_cents&apos;)
    def validate_totals(cls, v):
        &quot;&quot;&quot;Ensure totals are reasonable&quot;&quot;&quot;
        if v &gt; 100_000_000_000:  # $1B in cents
            raise ValueError(&quot;Total exceeds reasonable maximum&quot;)
        return v
```

### Item 19 - Financial Performance Representations

```python
class DisclosureType(str, Enum):
    HISTORICAL = &quot;Historical&quot;
    PROJECTED = &quot;Projected&quot;
    NONE = &quot;None&quot;
    MIXED = &quot;Mixed&quot;

class FPRBase(BaseModel):
    &quot;&quot;&quot;Base model for Item 19 FPR&quot;&quot;&quot;
    disclosure_type: Optional[DisclosureType] = None
    methodology: Optional[str] = None
    sample_size: Optional[int] = Field(None, gt=0)
    sample_description: Optional[str] = None
    time_period: Optional[str] = None
    
    # Revenue metrics
    average_revenue_cents: Optional[int] = Field(None, ge=0)
    median_revenue_cents: Optional[int] = Field(None, ge=0)
    low_revenue_cents: Optional[int] = Field(None, ge=0)
    high_revenue_cents: Optional[int] = Field(None, ge=0)
    
    # Profit metrics
    average_profit_cents: Optional[int] = None
    median_profit_cents: Optional[int] = None
    profit_margin_percentage: Optional[float] = Field(None, ge=-100, le=100)
    
    # Complex data
    additional_metrics: Dict[str, Any] = Field(default_factory=dict)
    tables_data: List[Dict[str, Any]] = Field(default_factory=list)
    
    disclaimers: Optional[str] = None
    
    @root_validator
    def validate_revenue_range(cls, values):
        &quot;&quot;&quot;Ensure revenue metrics are consistent&quot;&quot;&quot;
        low = values.get(&apos;low_revenue_cents&apos;)
        high = values.get(&apos;high_revenue_cents&apos;)
        avg = values.get(&apos;average_revenue_cents&apos;)
        median = values.get(&apos;median_revenue_cents&apos;)
        
        if all(v is not None for v in [low, high, avg]):
            if not (low &lt;= avg &lt;= high):
                raise ValueError(&quot;Average revenue must be between low and high&quot;)
        
        if all(v is not None for v in [low, high, median]):
            if not (low &lt;= median &lt;= high):
                raise ValueError(&quot;Median revenue must be between low and high&quot;)
        
        return values
    
    @validator(&apos;profit_margin_percentage&apos;)
    def validate_profit_margin(cls, v):
        &quot;&quot;&quot;Flag unusual profit margins&quot;&quot;&quot;
        if v is not None:
            if v &lt; -50:
                # Flag for review but don&apos;t reject
                pass
            if v &gt; 50:
                # Very high margin - flag for review
                pass
        return v

class FPR(FPRBase):
    &quot;&quot;&quot;FPR with section reference&quot;&quot;&quot;
    section_id: UUID
    created_at: datetime
    
    class Config:
        orm_mode = True
```

### Item 20 - Outlet Information

```python
class OutletType(str, Enum):
    FRANCHISED = &quot;Franchised&quot;
    COMPANY_OWNED = &quot;Company-Owned&quot;

class OutletSummaryBase(BaseModel):
    &quot;&quot;&quot;Base model for outlet summary by year&quot;&quot;&quot;
    fiscal_year: int = Field(..., ge=1900, le=2100)
    outlet_type: OutletType
    
    # Counts
    count_start: int = Field(..., ge=0)
    opened: int = Field(default=0, ge=0)
    closed: int = Field(default=0, ge=0)
    transferred_in: int = Field(default=0, ge=0)
    transferred_out: int = Field(default=0, ge=0)
    count_end: int = Field(..., ge=0)
    
    @root_validator
    def validate_outlet_math(cls, values):
        &quot;&quot;&quot;Ensure outlet counts balance&quot;&quot;&quot;
        start = values.get(&apos;count_start&apos;, 0)
        opened = values.get(&apos;opened&apos;, 0)
        closed = values.get(&apos;closed&apos;, 0)
        transferred_in = values.get(&apos;transferred_in&apos;, 0)
        transferred_out = values.get(&apos;transferred_out&apos;, 0)
        end = values.get(&apos;count_end&apos;, 0)
        
        calculated_end = start + opened - closed + transferred_in - transferred_out
        
        if calculated_end != end:
            raise ValueError(
                f&quot;Outlet math doesn&apos;t balance: &quot;
                f&quot;{start} + {opened} - {closed} + {transferred_in} - {transferred_out} &quot;
                f&quot;= {calculated_end}, but count_end = {end}&quot;
            )
        
        return values
    
    @validator(&apos;fiscal_year&apos;)
    def validate_reasonable_year(cls, v):
        &quot;&quot;&quot;Ensure year is reasonable&quot;&quot;&quot;
        current_year = datetime.now().year
        if v &gt; current_year + 1:
            raise ValueError(f&quot;Fiscal year {v} is in the future&quot;)
        return v

class OutletSummary(OutletSummaryBase):
    &quot;&quot;&quot;Outlet summary with section reference&quot;&quot;&quot;
    section_id: UUID
    
    class Config:
        orm_mode = True

class StateCountBase(BaseModel):
    &quot;&quot;&quot;Base model for state-by-state outlet counts&quot;&quot;&quot;
    state_code: str = Field(..., regex=&quot;^[A-Z]{2}$&quot;)
    franchised_count: int = Field(default=0, ge=0)
    company_owned_count: int = Field(default=0, ge=0)
    
    @property
    def total_count(self) -&gt; int:
        return self.franchised_count + self.company_owned_count
    
    @validator(&apos;state_code&apos;)
    def validate_state_code(cls, v):
        &quot;&quot;&quot;Ensure valid US state code&quot;&quot;&quot;
        valid_states = {
            &apos;AL&apos;, &apos;AK&apos;, &apos;AZ&apos;, &apos;AR&apos;, &apos;CA&apos;, &apos;CO&apos;, &apos;CT&apos;, &apos;DE&apos;, &apos;FL&apos;, &apos;GA&apos;,
            &apos;HI&apos;, &apos;ID&apos;, &apos;IL&apos;, &apos;IN&apos;, &apos;IA&apos;, &apos;KS&apos;, &apos;KY&apos;, &apos;LA&apos;, &apos;ME&apos;, &apos;MD&apos;,
            &apos;MA&apos;, &apos;MI&apos;, &apos;MN&apos;, &apos;MS&apos;, &apos;MO&apos;, &apos;MT&apos;, &apos;NE&apos;, &apos;NV&apos;, &apos;NH&apos;, &apos;NJ&apos;,
            &apos;NM&apos;, &apos;NY&apos;, &apos;NC&apos;, &apos;ND&apos;, &apos;OH&apos;, &apos;OK&apos;, &apos;OR&apos;, &apos;PA&apos;, &apos;RI&apos;, &apos;SC&apos;,
            &apos;SD&apos;, &apos;TN&apos;, &apos;TX&apos;, &apos;UT&apos;, &apos;VT&apos;, &apos;VA&apos;, &apos;WA&apos;, &apos;WV&apos;, &apos;WI&apos;, &apos;WY&apos;,
            &apos;DC&apos;, &apos;PR&apos;, &apos;VI&apos;, &apos;GU&apos;, &apos;AS&apos;, &apos;MP&apos;  # Include territories
        }
        if v not in valid_states:
            raise ValueError(f&quot;Invalid state code: {v}&quot;)
        return v

class StateCount(StateCountBase):
    &quot;&quot;&quot;State count with section reference&quot;&quot;&quot;
    section_id: UUID
    
    class Config:
        orm_mode = True

class OutletStateSummary(BaseModel):
    &quot;&quot;&quot;Aggregated outlet information&quot;&quot;&quot;
    section_id: UUID
    states: List[StateCount]
    total_franchised: int = 0
    total_company_owned: int = 0
    
    @root_validator
    def calculate_totals(cls, values):
        &quot;&quot;&quot;Calculate totals from states&quot;&quot;&quot;
        states = values.get(&apos;states&apos;, [])
        values[&apos;total_franchised&apos;] = sum(s.franchised_count for s in states)
        values[&apos;total_company_owned&apos;] = sum(s.company_owned_count for s in states)
        return values
```

### Item 21 - Financial Statements

```python
class AuditOpinion(str, Enum):
    UNQUALIFIED = &quot;Unqualified&quot;
    QUALIFIED = &quot;Qualified&quot;
    ADVERSE = &quot;Adverse&quot;
    DISCLAIMER = &quot;Disclaimer&quot;

class FinancialsBase(BaseModel):
    &quot;&quot;&quot;Base model for financial statements&quot;&quot;&quot;
    fiscal_year: Optional[int] = Field(None, ge=1900, le=2100)
    fiscal_year_end: Optional[date] = None
    
    # Income Statement
    total_revenue_cents: Optional[int] = Field(None, ge=0)
    franchise_revenue_cents: Optional[int] = Field(None, ge=0)
    cost_of_goods_cents: Optional[int] = Field(None, ge=0)
    gross_profit_cents: Optional[int] = None
    operating_expenses_cents: Optional[int] = Field(None, ge=0)
    operating_income_cents: Optional[int] = None
    net_income_cents: Optional[int] = None
    
    # Balance Sheet
    total_assets_cents: Optional[int] = Field(None, ge=0)
    current_assets_cents: Optional[int] = Field(None, ge=0)
    total_liabilities_cents: Optional[int] = Field(None, ge=0)
    current_liabilities_cents: Optional[int] = Field(None, ge=0)
    total_equity_cents: Optional[int] = None
    
    # Audit info
    auditor_name: Optional[str] = None
    audit_opinion: Optional[AuditOpinion] = None
    
    @root_validator
    def validate_accounting_equations(cls, values):
        &quot;&quot;&quot;Validate basic accounting equations where possible&quot;&quot;&quot;
        # Revenue - COGS = Gross Profit
        revenue = values.get(&apos;total_revenue_cents&apos;)
        cogs = values.get(&apos;cost_of_goods_cents&apos;)
        gross = values.get(&apos;gross_profit_cents&apos;)
        
        if all(v is not None for v in [revenue, cogs, gross]):
            calculated_gross = revenue - cogs
            if abs(calculated_gross - gross) &gt; 100:  # Allow $1 rounding error
                raise ValueError(
                    f&quot;Gross profit calculation error: &quot;
                    f&quot;{revenue} - {cogs} = {calculated_gross}, not {gross}&quot;
                )
        
        # Assets = Liabilities + Equity
        assets = values.get(&apos;total_assets_cents&apos;)
        liabilities = values.get(&apos;total_liabilities_cents&apos;)
        equity = values.get(&apos;total_equity_cents&apos;)
        
        if all(v is not None for v in [assets, liabilities, equity]):
            calculated_equity = assets - liabilities
            if abs(calculated_equity - equity) &gt; 100:  # Allow $1 rounding error
                raise ValueError(
                    f&quot;Balance sheet doesn&apos;t balance: &quot;
                    f&quot;{assets} - {liabilities} = {calculated_equity}, not {equity}&quot;
                )
        
        return values
    
    @root_validator
    def validate_ratios(cls, values):
        &quot;&quot;&quot;Validate financial ratios are reasonable&quot;&quot;&quot;
        # Current ratio check
        current_assets = values.get(&apos;current_assets_cents&apos;)
        current_liabilities = values.get(&apos;current_liabilities_cents&apos;)
        
        if current_assets is not None and current_liabilities is not None:
            if current_liabilities &gt; 0:
                current_ratio = current_assets / current_liabilities
                if current_ratio &lt; 0.1:
                    # Flag for review - very low liquidity
                    pass
        
        # Franchise revenue shouldn&apos;t exceed total revenue
        total_rev = values.get(&apos;total_revenue_cents&apos;)
        franchise_rev = values.get(&apos;franchise_revenue_cents&apos;)
        
        if total_rev is not None and franchise_rev is not None:
            if franchise_rev &gt; total_rev:
                raise ValueError(&quot;Franchise revenue cannot exceed total revenue&quot;)
        
        return values

class Financials(FinancialsBase):
    &quot;&quot;&quot;Financial statements with section reference&quot;&quot;&quot;
    section_id: UUID
    created_at: datetime
    
    class Config:
        orm_mode = True
```

### Generic JSON Storage Models

```python
class ItemJSONBase(BaseModel):
    &quot;&quot;&quot;Base model for generic item storage&quot;&quot;&quot;
    item_no: int = Field(..., ge=0, le=24)
    data: Dict[str, Any] = Field(default_factory=dict)
    schema_version: str = Field(default=&quot;1.0&quot;)
    
    @validator(&apos;data&apos;)
    def validate_data_not_empty(cls, v):
        &quot;&quot;&quot;Ensure data has content&quot;&quot;&quot;
        if not v:
            raise ValueError(&quot;Data cannot be empty&quot;)
        return v

class ItemJSON(ItemJSONBase):
    &quot;&quot;&quot;JSON storage with metadata&quot;&quot;&quot;
    section_id: UUID
    validated: bool = False
    validation_errors: Optional[List[Dict[str, Any]]] = None
    created_at: datetime
    updated_at: datetime
    
    class Config:
        orm_mode = True

# Item-specific JSON schemas
class Item1Schema(BaseModel):
    &quot;&quot;&quot;The Franchisor and Any Parents, Predecessors, and Affiliates&quot;&quot;&quot;
    franchisor_info: Dict[str, Any]
    parent_companies: List[Dict[str, str]]
    predecessors: List[Dict[str, str]]
    affiliates: List[Dict[str, str]]

class Item2Schema(BaseModel):
    &quot;&quot;&quot;Business Experience&quot;&quot;&quot;
    executives: List[Dict[str, Any]]  # name, position, experience

class Item3Schema(BaseModel):
    &quot;&quot;&quot;Litigation&quot;&quot;&quot;
    cases: List[Dict[str, Any]]  # case_name, court, date, status, description

class Item4Schema(BaseModel):
    &quot;&quot;&quot;Bankruptcy&quot;&quot;&quot;
    bankruptcies: List[Dict[str, Any]]  # person, date, court, case_number

# ... continue for other items
```

## Operational Models

### Scraping and Metadata

```python
class ScrapeMetadataBase(BaseModel):
    &quot;&quot;&quot;Base model for scrape metadata&quot;&quot;&quot;
    fdd_id: UUID
    source_name: str  # &apos;MN&apos;, &apos;WI&apos;, etc.
    source_url: str
    filing_metadata: Dict[str, Any] = Field(default_factory=dict)
    prefect_run_id: Optional[UUID] = None
    
    @validator(&apos;source_name&apos;)
    def validate_source(cls, v):
        &quot;&quot;&quot;Ensure known source&quot;&quot;&quot;
        valid_sources = {&apos;MN&apos;, &apos;WI&apos;, &apos;CA&apos;, &apos;WA&apos;, &apos;MD&apos;, &apos;VA&apos;, &apos;IL&apos;, &apos;MI&apos;, &apos;ND&apos;}
        if v not in valid_sources:
            raise ValueError(f&quot;Unknown source: {v}&quot;)
        return v
    
    @validator(&apos;source_url&apos;)
    def validate_url(cls, v):
        &quot;&quot;&quot;Basic URL validation&quot;&quot;&quot;
        if not v.startswith((&apos;http://&apos;, &apos;https://&apos;)):
            raise ValueError(&quot;Invalid URL format&quot;)
        return v

class ScrapeMetadata(ScrapeMetadataBase):
    &quot;&quot;&quot;Scrape metadata with timestamps&quot;&quot;&quot;
    id: UUID
    scraped_at: datetime
    
    class Config:
        orm_mode = True
```

### Pipeline Logging

```python
class LogLevel(str, Enum):
    DEBUG = &quot;DEBUG&quot;
    INFO = &quot;INFO&quot;
    WARNING = &quot;WARNING&quot;
    ERROR = &quot;ERROR&quot;
    CRITICAL = &quot;CRITICAL&quot;

class PipelineLogBase(BaseModel):
    &quot;&quot;&quot;Base model for pipeline logs&quot;&quot;&quot;
    prefect_run_id: Optional[UUID] = None
    task_name: Optional[str] = None
    level: LogLevel
    message: str
    context: Dict[str, Any] = Field(default_factory=dict)
    
    @validator(&apos;message&apos;)
    def validate_message_length(cls, v):
        &quot;&quot;&quot;Ensure message isn&apos;t too long&quot;&quot;&quot;
        if len(v) &gt; 10000:
            return v[:10000] + &quot;... (truncated)&quot;
        return v

class PipelineLog(PipelineLogBase):
    &quot;&quot;&quot;Pipeline log with metadata&quot;&quot;&quot;
    id: UUID
    created_at: datetime
    
    class Config:
        orm_mode = True
```

## Composite Models and Views

```python
class FDDExtractionProgress(BaseModel):
    &quot;&quot;&quot;View model for extraction progress&quot;&quot;&quot;
    fdd_id: UUID
    franchise_id: UUID
    canonical_name: str
    total_sections: int
    extracted_sections: int
    failed_sections: int
    needs_review: int
    success_rate: float
    
    @property
    def is_complete(self) -&gt; bool:
        return self.extracted_sections == self.total_sections
    
    @property
    def has_failures(self) -&gt; bool:
        return self.failed_sections &gt; 0

class FranchisorFDDSummary(BaseModel):
    &quot;&quot;&quot;Summary of FDDs for a franchisor&quot;&quot;&quot;
    franchisor: Franchisor
    total_fdds: int
    latest_fdd: Optional[FDD] = None
    states_filed: List[str]
    years_available: List[int]
    
    @property
    def filing_history_years(self) -&gt; int:
        if self.years_available:
            return max(self.years_available) - min(self.years_available) + 1
        return 0
```

## Utility Functions

```python
def cents_to_dollars(cents: Optional[int]) -&gt; Optional[float]:
    &quot;&quot;&quot;Convert cents to dollars with proper rounding&quot;&quot;&quot;
    if cents is None:
        return None
    return round(cents / 100, 2)

def dollars_to_cents(dollars: Optional[float]) -&gt; Optional[int]:
    &quot;&quot;&quot;Convert dollars to cents&quot;&quot;&quot;
    if dollars is None:
        return None
    return int(round(dollars * 100))

def validate_state_total(state_counts: List[StateCount], 
                        outlet_summaries: List[OutletSummary]) -&gt; bool:
    &quot;&quot;&quot;Validate state counts match outlet summary totals&quot;&quot;&quot;
    state_total_franchised = sum(s.franchised_count for s in state_counts)
    state_total_company = sum(s.company_owned_count for s in state_counts)
    
    # Get most recent year from outlet summaries
    if outlet_summaries:
        recent_year = max(o.fiscal_year for o in outlet_summaries)
        year_summaries = [o for o in outlet_summaries if o.fiscal_year == recent_year]
        
        outlet_franchised = sum(
            o.count_end for o in year_summaries 
            if o.outlet_type == OutletType.FRANCHISED
        )
        outlet_company = sum(
            o.count_end for o in year_summaries 
            if o.outlet_type == OutletType.COMPANY_OWNED
        )
        
        return (state_total_franchised == outlet_franchised and 
                state_total_company == outlet_company)
    
    return True
```

## Configuration

```python
class ValidationConfig:
    &quot;&quot;&quot;Global validation configuration&quot;&quot;&quot;
    
    # Maximum amounts (in cents)
    MAX_FEE_AMOUNT = 10_000_000_00  # $10M
    MAX_INVESTMENT_AMOUNT = 100_000_000_00  # $100M
    MAX_REVENUE_AMOUNT = 10_000_000_000_00  # $10B
    
    # Percentage limits
    MAX_ROYALTY_PERCENTAGE = 50.0
    MAX_MARKETING_PERCENTAGE = 20.0
    
    # Business rules
    REQUIRE_AUDIT_ABOVE_REVENUE = 50_000_000_00  # $50M
    FLAG_NEGATIVE_EQUITY_THRESHOLD = -10_000_000_00  # -$10M
    
    # Data quality thresholds
    MIN_SAMPLE_SIZE_FOR_FPR = 5
    MAX_YEARS_HISTORICAL_DATA = 10
```

---

**Note**: All models include Pydantic&apos;s built-in JSON serialization support. Use `.dict()` for dictionary conversion and `.json()` for JSON string serialization. Models with `orm_mode = True` can be initialized directly from SQLAlchemy ORM objects.</file><file path="docs/system_overview.md"># System Overview

## Executive Summary

The FDD Pipeline is an automated document intelligence system that transforms unstructured Franchise Disclosure Documents (FDDs) into structured, queryable data. It combines web scraping, document analysis, AI-powered extraction, and cloud storage to create a comprehensive franchise intelligence platform.

## System Goals

### Primary Objectives
1. **Automated Acquisition**: Continuously monitor and download FDDs from state regulatory portals
2. **Intelligent Processing**: Extract structured data from 25 FDD sections (Intro + Items 1-23 + Appendix)
3. **Data Quality**: Ensure high accuracy through multi-tier validation
4. **Scalability**: Handle thousands of documents efficiently
5. **Accessibility**: Provide clean APIs for downstream applications

### Key Metrics
- **Processing Throughput**: 100+ FDDs per day
- **Extraction Accuracy**: &gt;95% for structured sections
- **System Uptime**: 99.5% availability
- **End-to-End Latency**: &lt;10 minutes per document

## High-Level Architecture

```mermaid
graph TB
    subgraph &quot;External Sources&quot;
        MN[Minnesota Portal]
        WI[Wisconsin Portal]
        FS[Future States]
    end
    
    subgraph &quot;FDD Pipeline Core&quot;
        subgraph &quot;Acquisition&quot;
            SC[State Scrapers]
            DM[Download Manager]
            DD[Dedup Engine]
        end
        
        subgraph &quot;Processing&quot;
            LA[Layout Analyzer&lt;br/&gt;MinerU]
            DS[Document Splitter]
            EE[Extraction Engine&lt;br/&gt;LLMs]
        end
        
        subgraph &quot;Validation&quot;
            SV[Schema Validator]
            BV[Business Rules]
            QS[Quality Scorer]
        end
    end
    
    subgraph &quot;Infrastructure&quot;
        GD[(Google Drive)]
        PG[(PostgreSQL&lt;br/&gt;Supabase)]
        PF[Prefect&lt;br/&gt;Orchestrator]
    end
    
    subgraph &quot;Consumers&quot;
        API[Internal API]
        EF[Edge Functions]
        UI[Web UI]
        AN[Analytics]
    end
    
    MN --&gt; SC
    WI --&gt; SC
    FS -.-&gt; SC
    
    SC --&gt; DM
    DM --&gt; DD
    DD --&gt; GD
    DD --&gt; PG
    
    GD --&gt; LA
    LA --&gt; DS
    DS --&gt; EE
    EE --&gt; SV
    SV --&gt; BV
    BV --&gt; QS
    QS --&gt; PG
    
    PF -.-&gt; SC
    PF -.-&gt; LA
    PF -.-&gt; EE
    
    PG --&gt; API
    PG --&gt; EF
    EF --&gt; UI
    API --&gt; AN
```

## Core Components

### 1. Acquisition Subsystem

**Purpose**: Automated collection of FDD documents from various sources.

**Key Components**:
- **State-Specific Scrapers**: Custom Playwright/httpx scripts per portal
- **Download Manager**: Handles file downloads with retry logic
- **Deduplication Engine**: Prevents duplicate processing using fuzzy matching

**Data Flow**:
1. Scrapers poll state portals on schedule
2. Extract metadata and download URLs
3. Download PDFs directly to Google Drive
4. Check for duplicates using embeddings
5. Register new documents in database

### 2. Document Processing Subsystem

**Purpose**: Transform PDFs into structured, section-based data.

**Processing Pipeline**:

```mermaid
sequenceDiagram
    participant PDF as Original PDF
    participant MinerU as MinerU API
    participant Splitter as Section Splitter
    participant LLM as LLM Extractor
    participant DB as Database
    
    PDF-&gt;&gt;MinerU: Send for analysis
    MinerU-&gt;&gt;Splitter: Return layout JSON
    Splitter-&gt;&gt;Splitter: Identify 25 sections (0-24)
    Splitter-&gt;&gt;LLM: Send section PDFs
    loop For each section
        LLM-&gt;&gt;LLM: Extract structured data
        LLM-&gt;&gt;DB: Store results
    end
```

**Key Technologies**:
- **MinerU**: AI-powered layout analysis
- **Section Detection**: Rule-based + ML hybrid approach
- **LLM Routing**: Model selection based on complexity

### 3. Validation Subsystem

**Purpose**: Ensure data quality and consistency.

**Validation Layers**:

| Layer | Purpose | Examples | Action |
|-------|---------|----------|--------|
| Schema | Structure validation | Required fields, types | Retry/Fail |
| Business | Domain rules | Sum validations, date logic | Flag/Review |
| Quality | Completeness | Missing data, OCR quality | Score/Alert |

### 4. Storage Architecture

**Document Storage (Google Drive)**:
```
/fdds/
├── /raw/                  # Original PDFs
│   ├── /mn/
│   └── /wi/
├── /processed/            # Segmented PDFs
│   └── /{franchise_id}/
│       └── /{year}/
│           ├── section_01.pdf
│           └── ...
└── /archive/              # Old/superseded
```

**Data Storage (PostgreSQL/Supabase)**:
- Normalized tables for Items 5, 6, 7, 19, 20, 21
- JSON storage for other sections
- Full audit trail and versioning

## System Interactions

### Synchronous Operations
1. **API Queries**: Real-time data access
2. **Status Checks**: Pipeline monitoring
3. **Manual Uploads**: Direct document submission

### Asynchronous Operations
1. **Scheduled Scraping**: Weekly portal checks
2. **Document Processing**: Queue-based extraction
3. **Batch Validation**: Nightly quality checks

## Security &amp; Compliance

### Data Security
- **Encryption**: TLS for all external communications
- **Authentication**: Service accounts with minimal permissions
- **Access Control**: Row-level security in database
- **Audit Trail**: Complete operation logging

### Compliance Considerations
- **Data Retention**: 7-year archive policy
- **Privacy**: No PII extraction or storage
- **Licensing**: Respect portal terms of service
- **Right to Deletion**: Support for data removal requests

## Scalability Design

### Horizontal Scaling Points
- **Scrapers**: Multiple agents per state
- **Processing**: Parallel section extraction
- **API**: Load-balanced endpoints

### Vertical Scaling Options
- **Database**: Auto-scaling with Supabase
- **LLM Calls**: Rate limiting and queuing
- **Storage**: Unlimited Google Drive capacity

### Performance Optimizations
1. **Caching**: Embedding vectors for deduplication
2. **Batching**: Group similar extraction tasks
3. **Compression**: Archive old documents
4. **Indexing**: Optimized database queries

## Monitoring &amp; Observability

### Key Metrics Dashboard

```mermaid
graph LR
    subgraph &quot;Acquisition Metrics&quot;
        A1[Documents/Day]
        A2[Success Rate]
        A3[Duplicate Rate]
    end
    
    subgraph &quot;Processing Metrics&quot;
        P1[Sections/Hour]
        P2[LLM Accuracy]
        P3[Token Usage]
    end
    
    subgraph &quot;System Health&quot;
        S1[API Latency]
        S2[Error Rate]
        S3[Queue Depth]
    end
```

### Alerting Triggers
- Pipeline failures
- Extraction accuracy &lt; 95%
- API response time &gt; 2s
- Storage usage &gt; 80%

## Disaster Recovery

### Backup Strategy
- **Database**: Daily automated backups
- **Documents**: Google Drive versioning
- **Code**: Git repository mirrors
- **Secrets**: Encrypted vault backup

### Recovery Procedures
1. **Data Loss**: Restore from latest backup
2. **Service Outage**: Failover to backup services
3. **Corruption**: Reprocess from raw documents
4. **Security Breach**: Rotate all credentials

## Future Architecture Evolution

### Phase 2 (6 months)
- Additional state portals
- Real-time processing pipeline
- Advanced deduplication ML
- Public API gateway

### Phase 3 (12 months)
- Multi-region deployment
- Custom extraction models
- Streaming data platform
- Enterprise integrations

## System Boundaries

### In Scope
- FDD document processing
- State portal scraping
- Structured data extraction
- API access layer

### Out of Scope
- Document generation
- Legal analysis
- Franchise recommendations
- Payment processing

## Technology Stack Summary

| Layer | Technology | Purpose |
|-------|------------|---------|
| Orchestration | Prefect | Workflow management |
| Web Scraping | Playwright | Browser automation |
| Storage | Google Drive | Document storage |
| Database | Supabase/PostgreSQL | Structured data |
| AI/ML | Gemini/Ollama | Text extraction |
| API | FastAPI | Internal services |
| Monitoring | Prefect + Custom | System observability |

---

For detailed component documentation, see:
- [Data Flow](data_flow.md) - Detailed pipeline stages
- [Technology Decisions](technology_decisions.md) - Architecture choices explained</file><file path="docs/troubleshooting.md"># Troubleshooting Guide

## Overview

This guide provides solutions for common issues encountered with the FDD Pipeline, including diagnostic procedures, resolution steps, and preventive measures.

## Quick Diagnostics

### Health Check Script
```bash
# Run comprehensive health check
python scripts/health_check.py

# Sample output:
# ✓ Prefect server: Connected
# ✓ Database: Connected (42ms latency)
# ✓ Google Drive: Authenticated
# ✓ Email: Configuration valid
# ✗ ChromeDriver: Not found in PATH
# ✓ Disk space: 15.2 GB available
# ✓ Python packages: All installed
```

### System Status Commands
```bash
# Check Prefect status
prefect server health-check
prefect deployment ls
prefect work-pool ls

# Check recent logs
tail -f logs/fdd_pipeline.log | grep ERROR

# Database connectivity
python -c &quot;from src.utils.database import test_connection; test_connection()&quot;

# Review recent errors
python scripts/error_summary.py --hours 24
```

## Common Issues and Solutions

### 1. Scraping Failures

#### Issue: &quot;Element not found&quot; errors
**Symptoms:**
- Selenium TimeoutException
- &quot;Unable to locate element&quot; in logs
- Specific departments failing consistently

**Diagnosis:**
```python
# Test specific department
python scripts/test_scrape.py --university mn --department &quot;Computer Science&quot; --verbose

# Check page structure
python scripts/analyze_page.py --url &quot;https://example.edu/directory&quot;
```

**Solutions:**
1. Update selectors:
   ```python
   # src/scrapers/element_selectors.py
   SELECTORS = {
       &apos;mn&apos;: {
           &apos;faculty_list&apos;: &apos;//div[@class=&quot;faculty-listing&quot;]&apos;,  # Old
           &apos;faculty_list&apos;: &apos;//div[@class=&quot;staff-directory&quot;]&apos;,  # New
       }
   }
   ```

2. Increase timeouts:
   ```python
   # config/scraper_config.py
   ELEMENT_TIMEOUT = 30  # Increase from 20
   PAGE_LOAD_TIMEOUT = 45  # Increase from 30
   ```

3. Add retry logic:
   ```python
   # src/scrapers/base_scraper.py
   @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=2))
   def find_element_safe(self, selector):
       return self.driver.find_element(By.XPATH, selector)
   ```

#### Issue: &quot;Access denied&quot; or rate limiting
**Symptoms:**
- 403 Forbidden responses
- &quot;Too many requests&quot; messages
- IP blocked temporarily

**Solutions:**
1. Implement delays:
   ```python
   # config/scraper_config.py
   REQUEST_DELAY = 2  # Seconds between requests
   DEPARTMENT_DELAY = 5  # Seconds between departments
   ```

2. Add user agent rotation:
   ```python
   # src/scrapers/browser_config.py
   USER_AGENTS = [
       &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36&apos;,
       &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36&apos;,
   ]
   
   options.add_argument(f&apos;user-agent={random.choice(USER_AGENTS)}&apos;)
   ```

3. Use proxy rotation (if necessary):
   ```python
   # config/proxy_config.py
   PROXY_LIST = [
       &apos;http://proxy1.example.com:8080&apos;,
       &apos;http://proxy2.example.com:8080&apos;,
   ]
   ```

### 2. Database Issues

#### Issue: Connection timeouts to Supabase
**Symptoms:**
- &quot;Connection timeout&quot; errors
- &quot;SSL connection has been closed unexpectedly&quot;
- Intermittent failures

**Diagnosis:**
```bash
# Test connection
python scripts/test_database.py

# Check network latency
ping your-project.supabase.co

# Verify credentials
python -c &quot;import os; print(os.getenv(&apos;SUPABASE_URL&apos;))&quot;
```

**Solutions:**
1. Implement connection pooling:
   ```python
   # src/utils/database.py
   from supabase import create_client
   from urllib3.util.retry import Retry
   
   class DatabaseConnection:
       def __init__(self):
           self.client = None
           self.retry_strategy = Retry(
               total=3,
               backoff_factor=1,
               status_forcelist=[429, 500, 502, 503, 504]
           )
       
       def get_client(self):
           if not self.client:
               self.client = create_client(
                   os.getenv(&apos;SUPABASE_URL&apos;),
                   os.getenv(&apos;SUPABASE_KEY&apos;)
               )
           return self.client
   ```

2. Add connection retry logic:
   ```python
   @retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
   def execute_query(self, query):
       return self.client.table(&apos;pipeline_logs&apos;).insert(query).execute()
   ```

3. Handle connection drops:
   ```python
   def safe_insert(self, data):
       try:
           return self.execute_query(data)
       except Exception as e:
           logger.error(f&quot;Database error: {e}&quot;)
           self.client = None  # Force reconnection
           return self.execute_query(data)
   ```

#### Issue: Data insertion failures
**Symptoms:**
- &quot;Duplicate key&quot; errors
- &quot;Invalid input syntax&quot; errors
- Partial data saved

**Solutions:**
1. Add data validation:
   ```python
   # src/utils/validators.py
   def validate_faculty_data(data):
       required_fields = [&apos;name&apos;, &apos;department&apos;, &apos;university&apos;]
       for field in required_fields:
           if field not in data or not data[field]:
               raise ValueError(f&quot;Missing required field: {field}&quot;)
       
       # Email validation
       if &apos;email&apos; in data and data[&apos;email&apos;]:
           if not re.match(r&apos;^[\w\.-]+@[\w\.-]+\.\w+$&apos;, data[&apos;email&apos;]):
               data[&apos;email&apos;] = None
       
       return data
   ```

2. Implement upsert logic:
   ```python
   # src/utils/database_operations.py
   def upsert_faculty(self, faculty_data):
       return self.client.table(&apos;faculty&apos;)\
           .upsert(
               faculty_data,
               on_conflict=&apos;email,university&apos;,
               returning=&apos;minimal&apos;
           ).execute()
   ```

### 3. Prefect Flow Issues

#### Issue: Flows not starting on schedule
**Symptoms:**
- Scheduled runs not appearing
- &quot;No workers available&quot; messages
- Deployments show as inactive

**Diagnosis:**
```bash
# Check deployment status
prefect deployment inspect minnesota-scrape/prod

# Verify schedules
prefect deployment schedule ls minnesota-scrape/prod

# Check worker status
prefect work-pool inspect fdd-local-pool
```

**Solutions:**
1. Restart worker:
   ```bash
   # Find and kill existing worker
   ps aux | grep &quot;prefect worker&quot;
   kill -TERM &lt;worker-pid&gt;
   
   # Start new worker
   prefect worker start --pool fdd-local-pool --limit 1
   ```

2. Fix timezone issues:
   ```bash
   # Update schedule with correct timezone
   prefect deployment schedule delete minnesota-scrape/prod &lt;schedule-id&gt;
   prefect deployment schedule create minnesota-scrape/prod \
     --cron &quot;0 2 * * 1&quot; \
     --timezone &quot;America/Chicago&quot;
   ```

3. Re-deploy flows:
   ```bash
   python deployments/deploy_mn_flow.py --force
   ```

#### Issue: Flow runs hanging or timing out
**Symptoms:**
- Flows stuck in &quot;Running&quot; state
- No logs being generated
- Worker unresponsive

**Solutions:**
1. Add flow timeouts:
   ```python
   # src/flows/scraper_flow.py
   from prefect import flow
   from datetime import timedelta
   
   @flow(
       name=&quot;minnesota-scrape&quot;,
       timeout_seconds=7200,  # 2 hours
       retries=2,
       retry_delay_seconds=300
   )
   def scrape_minnesota():
       pass
   ```

2. Implement task-level timeouts:
   ```python
   @task(timeout_seconds=300)  # 5 minutes per department
   def scrape_department(url, department):
       pass
   ```

3. Add progress monitoring:
   ```python
   @task
   def scrape_with_progress(departments):
       for i, dept in enumerate(departments):
           logger.info(f&quot;Processing {i+1}/{len(departments)}: {dept}&quot;)
           yield scrape_department(dept)
   ```

### 4. Google Drive Issues

#### Issue: Authentication failures
**Symptoms:**
- &quot;Invalid credentials&quot; errors
- &quot;Token has been expired or revoked&quot;
- 401 Unauthorized responses

**Solutions:**
1. Refresh service account:
   ```bash
   # Verify service account file
   python scripts/test_google_auth.py
   
   # Check permissions
   python scripts/check_drive_permissions.py --folder-id $GOOGLE_DRIVE_FOLDER_ID
   ```

2. Re-authenticate:
   ```python
   # src/utils/google_drive.py
   def get_drive_service():
       creds = service_account.Credentials.from_service_account_file(
           os.getenv(&apos;GOOGLE_SERVICE_ACCOUNT_PATH&apos;),
           scopes=[&apos;https://www.googleapis.com/auth/drive&apos;]
       )
       return build(&apos;drive&apos;, &apos;v3&apos;, credentials=creds)
   ```

#### Issue: Upload failures
**Symptoms:**
- &quot;Insufficient permissions&quot; errors
- Timeouts during upload
- Files not appearing in Drive

**Solutions:**
1. Check folder permissions:
   ```python
   def verify_folder_access(folder_id):
       service = get_drive_service()
       try:
           folder = service.files().get(fileId=folder_id).execute()
           print(f&quot;Folder accessible: {folder[&apos;name&apos;]}&quot;)
           return True
       except Exception as e:
           print(f&quot;Cannot access folder: {e}&quot;)
           return False
   ```

2. Implement chunked uploads:
   ```python
   # src/utils/drive_upload.py
   from googleapiclient.http import MediaFileUpload
   
   def upload_large_file(file_path, folder_id):
       media = MediaFileUpload(
           file_path,
           mimetype=&apos;text/csv&apos;,
           resumable=True,
           chunksize=1024*1024  # 1MB chunks
       )
       
       request = service.files().create(
           body=file_metadata,
           media_body=media
       )
       
       response = None
       while response is None:
           status, response = request.next_chunk()
           if status:
               print(f&quot;Upload {int(status.progress() * 100)}%&quot;)
   ```

### 5. Performance Issues

#### Issue: Slow scraping performance
**Symptoms:**
- Departments taking &gt;1 minute each
- Total runtime &gt;3 hours
- Memory usage increasing

**Diagnosis:**
```python
# Profile scraping performance
python -m cProfile -o profile.stats scripts/test_scrape.py
python scripts/analyze_profile.py profile.stats
```

**Solutions:**
1. Implement parallel processing:
   ```python
   # src/scrapers/parallel_scraper.py
   from concurrent.futures import ThreadPoolExecutor
   
   def scrape_departments_parallel(departments, max_workers=4):
       with ThreadPoolExecutor(max_workers=max_workers) as executor:
           futures = []
           for dept in departments:
               future = executor.submit(scrape_department, dept)
               futures.append(future)
           
           results = []
           for future in as_completed(futures):
               try:
                   result = future.result(timeout=300)
                   results.append(result)
               except TimeoutError:
                   logger.error(f&quot;Department scrape timed out&quot;)
       
       return results
   ```

2. Optimize selectors:
   ```python
   # Use CSS selectors when faster
   # Instead of: //div[@class=&apos;faculty&apos;]//span[@class=&apos;name&apos;]
   # Use: div.faculty span.name
   
   def get_faculty_names(self):
       return self.driver.find_elements(By.CSS_SELECTOR, &quot;div.faculty span.name&quot;)
   ```

3. Cache static data:
   ```python
   # src/utils/cache.py
   from functools import lru_cache
   
   @lru_cache(maxsize=100)
   def get_department_list(university):
       # Cache department lists for 24 hours
       return scrape_department_list(university)
   ```

### 6. Email Alert Issues

#### Issue: Alerts not being sent
**Symptoms:**
- No email notifications for failures
- &quot;Authentication failed&quot; in logs
- SMTP connection errors

**Solutions:**
1. Test email configuration:
   ```bash
   python scripts/test_email.py --recipient test@example.com
   ```

2. Update SMTP settings:
   ```python
   # For Gmail with app password
   SMTP_HOST=smtp.gmail.com
   SMTP_PORT=587
   SMTP_USERNAME=your-email@gmail.com
   SMTP_PASSWORD=your-app-specific-password  # Not regular password
   ```

3. Implement fallback notification:
   ```python
   # src/utils/notifications.py
   def send_notification(subject, body):
       try:
           send_email(subject, body)
       except Exception as e:
           logger.error(f&quot;Email failed: {e}&quot;)
           # Fallback to local logging
           with open(&apos;alerts.log&apos;, &apos;a&apos;) as f:
               f.write(f&quot;{datetime.now()}: {subject}\n{body}\n\n&quot;)
   ```

## Advanced Troubleshooting

### Memory Leaks

**Detection:**
```python
# scripts/memory_monitor.py
import psutil
import os

def monitor_memory():
    process = psutil.Process(os.getpid())
    while True:
        mem_info = process.memory_info()
        print(f&quot;RSS: {mem_info.rss / 1024 / 1024:.2f} MB&quot;)
        time.sleep(10)
```

**Solutions:**
1. Properly close browser instances:
   ```python
   def scrape_with_cleanup(url):
       driver = None
       try:
           driver = create_driver()
           # Scraping logic
       finally:
           if driver:
               driver.quit()
   ```

2. Clear large objects:
   ```python
   def process_large_dataset(data):
       # Process data
       results = analyze_data(data)
       
       # Clear reference
       del data
       gc.collect()
       
       return results
   ```

### Debugging Techniques

#### Enable Debug Logging
```python
# config/logging_config.py
import logging

# Set all loggers to DEBUG
logging.basicConfig(level=logging.DEBUG)

# Enable Selenium debug logs
logging.getLogger(&apos;selenium&apos;).setLevel(logging.DEBUG)

# Enable database query logs
logging.getLogger(&apos;supabase&apos;).setLevel(logging.DEBUG)
```

#### Interactive Debugging
```python
# scripts/debug_scraper.py
import pdb
from src.scrapers import MinnesotaScraper

def debug_scrape():
    scraper = MinnesotaScraper()
    
    # Set breakpoint
    pdb.set_trace()
    
    # Step through scraping
    results = scraper.scrape_department(&quot;Computer Science&quot;)
    
    return results

if __name__ == &quot;__main__&quot;:
    debug_scrape()
```

#### Network Debugging
```bash
# Monitor network requests
mitmdump -s scripts/log_requests.py

# Check DNS resolution
nslookup your-project.supabase.co

# Test connectivity
curl -I https://your-project.supabase.co
```

## Prevention Strategies

### 1. Automated Testing
```python
# tests/test_scrapers.py
import pytest
from src.scrapers import MinnesotaScraper

@pytest.fixture
def scraper():
    return MinnesotaScraper()

def test_department_list(scraper):
    departments = scraper.get_departments()
    assert len(departments) &gt; 0
    assert all(&apos;name&apos; in dept for dept in departments)

def test_faculty_scrape(scraper):
    faculty = scraper.scrape_department(&quot;Computer Science&quot;)
    assert len(faculty) &gt; 0
    assert all(&apos;email&apos; in f for f in faculty)
```

### 2. Health Monitoring
```python
# scripts/continuous_monitor.py
#!/usr/bin/env python3
import time
from datetime import datetime, timedelta

def monitor_pipeline_health():
    while True:
        try:
            # Check last successful run
            check_last_run()
            
            # Monitor error rate
            check_error_rate()
            
            # Verify system resources
            check_system_health()
            
        except Exception as e:
            send_alert(f&quot;Monitor failed: {e}&quot;)
        
        time.sleep(300)  # Check every 5 minutes
```

### 3. Graceful Degradation
```python
# src/scrapers/resilient_scraper.py
class ResilientScraper:
    def scrape_all_departments(self):
        results = []
        failed_departments = []
        
        for dept in self.get_departments():
            try:
                data = self.scrape_department(dept)
                results.extend(data)
            except Exception as e:
                logger.error(f&quot;Failed to scrape {dept}: {e}&quot;)
                failed_departments.append(dept)
                continue  # Continue with next department
        
        # Report partial success
        if failed_departments:
            self.report_failures(failed_departments)
        
        return results
```

## Emergency Procedures

### Complete System Failure
1. **Immediate Actions:**
   ```bash
   # Stop all processes
   pkill -f prefect
   pkill -f python
   
   # Check system resources
   df -h
   free -m
   ps aux | sort -nrk 3,3 | head -10
   ```

2. **Diagnostic Steps:**
   ```bash
   # Review recent errors
   tail -n 1000 logs/fdd_pipeline_errors.log
   
   # Check database connectivity
   psql $DATABASE_URL -c &quot;SELECT 1&quot;
   
   # Verify external services
   curl -I https://www.umn.edu
   curl -I https://www.wisconsin.edu
   ```

3. **Recovery Steps:**
   ```bash
   # Clear temporary files
   rm -rf /tmp/selenium*
   rm -rf logs/*.log.*
   
   # Reset Prefect
   prefect server database reset -y
   
   # Redeploy flows
   ./scripts/redeploy_all.sh
   
   # Start fresh
   prefect server start &amp;
   prefect worker start --pool fdd-local-pool &amp;
   ```

### Data Recovery
```python
# scripts/recover_partial_data.py
def recover_incomplete_scrape():
    # Find incomplete runs
    incomplete = db.query(&quot;&quot;&quot;
        SELECT DISTINCT flow_run_id, university
        FROM pipeline_logs
        WHERE log_type = &apos;flow_started&apos;
        AND flow_run_id NOT IN (
            SELECT flow_run_id 
            FROM pipeline_logs 
            WHERE log_type = &apos;flow_completed&apos;
        )
        AND created_at &gt; NOW() - INTERVAL &apos;24 hours&apos;
    &quot;&quot;&quot;)
    
    # Attempt to recover data
    for run in incomplete:
        partial_data = db.query(f&quot;&quot;&quot;
            SELECT * FROM scraped_data
            WHERE flow_run_id = &apos;{run[&apos;flow_run_id&apos;]}&apos;
        &quot;&quot;&quot;)
        
        if partial_data:
            save_to_backup(partial_data)
            logger.info(f&quot;Recovered {len(partial_data)} records from {run[&apos;flow_run_id&apos;]}&quot;)
```

## Support Escalation

### Level 1: Self-Service
- Review this troubleshooting guide
- Check recent deployments for changes
- Run diagnostic scripts
- Review monitoring dashboards

### Level 2: Team Support
- Post in #fdd-pipeline-support Slack channel
- Include error logs and diagnostic output
- Tag on-call engineer if urgent

### Level 3: External Support
- Prefect Cloud support (for Prefect issues)
- Supabase support (for database issues)
- Google Cloud support (for Drive API issues)

### Creating Support Tickets
Include:
1. Error messages and stack traces
2. Time of occurrence
3. Recent changes or deployments
4. Diagnostic script output
5. Steps to reproduce
6. Impact assessment

## Appendix: Useful Commands

### Quick Diagnostics
```bash
# One-liner health check
python -c &quot;from scripts.health_check import run_all_checks; run_all_checks()&quot;

# Recent errors summary
grep -E &quot;(ERROR|CRITICAL)&quot; logs/fdd_pipeline.log | tail -20 | cut -d&apos; &apos; -f5- | sort | uniq -c

# Performance check
python -c &quot;from src.utils.database import get_metrics; print(get_metrics(&apos;24h&apos;))&quot;

# Test specific university
python -m src.scrapers.test_scraper --university mn --quick

# Force cleanup
python scripts/cleanup.py --force --all
```

### Recovery Commands
```bash
# Reset stuck flow
prefect flow-run cancel &lt;flow-run-id&gt;

# Clear queue
prefect work-queue clear fdd-local-queue

# Restart from checkpoint
python scripts/resume_scrape.py --flow-run-id &lt;id&gt; --from-checkpoint

# Emergency data export
python scripts/export_all_data.py --format csv --output /backup/</file><file path="docs/validation_rules.md"># Validation Rules Documentation

## Overview

The FDD Pipeline implements a three-tier validation system to ensure data quality and consistency:

1. **Schema Validation** - Pydantic models enforce data types, formats, and basic constraints
2. **Business Rules** - Domain-specific logic validates relationships and calculations
3. **Quality Checks** - Statistical and anomaly detection for data quality assurance

## Validation Tiers

### Tier 1: Schema Validation (Automatic)

Applied automatically when creating or updating records through Pydantic models.

| Category | Rules | Implementation |
|----------|-------|----------------|
| **Data Types** | All fields must match defined types | Pydantic type annotations |
| **Required Fields** | Non-optional fields must be present | Pydantic Field() definitions |
| **Format Validation** | Strings match expected patterns | Regex validators |
| **Range Validation** | Numbers within acceptable bounds | Field constraints (ge, le, gt, lt) |
| **Enum Validation** | Values from predefined sets | Enum classes |

### Tier 2: Business Rules (Contextual)

Applied during data processing to ensure business logic compliance.

| Category | Rules | Severity |
|----------|-------|----------|
| **Cross-field Dependencies** | Related fields must be consistent | ERROR |
| **Calculation Validation** | Totals and formulas must balance | ERROR |
| **Temporal Consistency** | Dates must follow logical order | WARNING |
| **Reference Integrity** | Foreign keys must exist | ERROR |
| **Domain Constraints** | Business-specific requirements | VARIES |

### Tier 3: Quality Checks (Statistical)

Applied periodically to identify anomalies and quality issues.

| Category | Rules | Action |
|----------|-------|--------|
| **Outlier Detection** | Values beyond statistical norms | FLAG_FOR_REVIEW |
| **Completeness** | Missing expected data | REPORT |
| **Consistency** | Cross-document validation | INVESTIGATE |
| **Trend Analysis** | Unusual patterns over time | ALERT |

## Field-Specific Validation Rules

### Currency Fields

All monetary values are stored as cents (integer) to avoid floating-point precision issues.

```python
# Validation Rules
- Must be non-negative (except where explicitly allowed)
- Maximum reasonable amounts enforced
- Conversion: dollars * 100 = cents
- Display: cents / 100 = dollars (rounded to 2 decimals)

# Examples
amount_cents: int = Field(..., ge=0)  # Non-negative
revenue_cents: int = Field(..., ge=0, le=10_000_000_000_00)  # Max $10B
```

### Percentage Fields

```python
# Validation Rules
- Range: 0.0 to 100.0 (unless explicitly allowing negative)
- Precision: Up to 2 decimal places
- Storage: As NUMERIC(5,2) in database

# Common Limits
royalty_percentage: float = Field(..., ge=0, le=50)  # Max 50%
marketing_percentage: float = Field(..., ge=0, le=20)  # Max 20%
profit_margin_percentage: float = Field(..., ge=-100, le=100)  # Can be negative
```

### Date Fields

```python
# Validation Rules
- Format: ISO 8601 (YYYY-MM-DD)
- Logical ordering enforced
- Future dates restricted where appropriate

# Examples
issue_date &lt;= amendment_date (if amendment exists)
fiscal_year_end must be within fiscal_year
issue_date cannot be more than 1 year in future
```

### State Codes

```python
# Validation Rules
- Format: 2-letter uppercase (e.g., &quot;CA&quot;, &quot;NY&quot;)
- Must be valid US state or territory code
- Includes: 50 states + DC + territories (PR, VI, GU, AS, MP)
```

## Item-Specific Validation Rules

### Item 5: Initial Fees

| Field | Validation Rules | Error Handling |
|-------|------------------|----------------|
| `fee_name` | - Required, non-empty&lt;br&gt;- Standardized names preferred | Normalize common variations |
| `amount_cents` | - Non-negative&lt;br&gt;- Maximum: $10M&lt;br&gt;- Must be &gt; 0 for required fees | Reject negative, flag high amounts |
| `refundable` | - If true, `refund_conditions` recommended | Warning if conditions missing |
| `due_at` | - Must be valid enum value&lt;br&gt;- Consistent with fee type | Validate against fee_name |

### Item 6: Other Fees

| Field | Validation Rules | Error Handling |
|-------|------------------|----------------|
| `amount_cents` OR `amount_percentage` | - Exactly one must be set&lt;br&gt;- Not both, not neither | ERROR: Reject invalid combination |
| `frequency` | - Required&lt;br&gt;- Must match fee type logic | Validate royalty=monthly, etc. |
| `minimum_cents`, `maximum_cents` | - Both optional&lt;br&gt;- If both set: max &gt;= min&lt;br&gt;- Non-negative | ERROR if max &lt; min |
| `calculation_basis` | - Required if percentage-based&lt;br&gt;- Must be logical for fee type | ERROR if missing for % fees |

**Special Rule**: Royalty fees should typically be percentage-based on gross/net sales.

### Item 7: Initial Investment

| Field | Validation Rules | Error Handling |
|-------|------------------|----------------|
| `low_cents`, `high_cents` | - At least one required&lt;br&gt;- If both: high &gt;= low&lt;br&gt;- Non-negative&lt;br&gt;- Maximum: $100M | ERROR if high &lt; low |
| `category` | - Required&lt;br&gt;- Standardize common names&lt;br&gt;- No duplicates per section | Normalize variations |
| **Total Investment** | - Sum of all categories&lt;br&gt;- Validate against Item 5 fees | Warning if inconsistent |

**Cross-validation**: Initial franchise fee in Item 7 must match Item 5.

### Item 19: Financial Performance Representations

| Field | Validation Rules | Error Handling |
|-------|------------------|----------------|
| `disclosure_type` | - If &quot;None&quot;, most fields should be null&lt;br&gt;- If set, require supporting data | Validate data presence |
| `sample_size` | - Required if metrics provided&lt;br&gt;- Minimum: 5 for credibility&lt;br&gt;- Must be &lt;= total outlets | Warning if &lt; 5 |
| **Revenue Metrics** | - low &lt;= average &lt;= high&lt;br&gt;- low &lt;= median &lt;= high&lt;br&gt;- All non-negative | ERROR if out of order |
| `profit_margin_percentage` | - Range: -100% to 100%&lt;br&gt;- Flag if &lt; -50% or &gt; 50% | Review extreme values |

### Item 20: Outlet Information

#### Outlet Summary Table

| Field | Validation Rules | Error Handling |
|-------|------------------|----------------|
| **Mathematical Balance** | `count_end = count_start + opened - closed + transferred_in - transferred_out` | ERROR if doesn&apos;t balance |
| `fiscal_year` | - Range: 1900-current+1&lt;br&gt;- Consecutive years expected | Warning if gaps |
| **All counts** | - Non-negative&lt;br&gt;- Reasonable maximums (&lt; 100,000) | Flag unusual values |

#### State Counts Table

| Field | Validation Rules | Error Handling |
|-------|------------------|----------------|
| `state_code` | - Valid US state/territory&lt;br&gt;- No duplicates per section | ERROR if invalid |
| **Total Validation** | Sum of all states must equal outlet summary totals | ERROR if mismatch |
| **Geographic Logic** | Adjacent states expected for chains | Flag unusual patterns |

### Item 21: Financial Statements

| Field | Validation Rules | Error Handling |
|-------|------------------|----------------|
| **Accounting Equations** | - Assets = Liabilities + Equity&lt;br&gt;- Revenue - COGS = Gross Profit&lt;br&gt;- Allow $1 rounding tolerance | ERROR if &gt; $1 difference |
| **Component Relationships** | - Current assets &lt;= Total assets&lt;br&gt;- Current liabilities &lt;= Total liabilities&lt;br&gt;- Franchise revenue &lt;= Total revenue | ERROR if violated |
| **Audit Requirements** | - If revenue &gt; $50M, expect auditor info&lt;br&gt;- Audit opinion required if auditor present | Warning if missing |
| **Negative Values** | - Assets/Liabilities: Non-negative&lt;br&gt;- Income/Equity: Can be negative&lt;br&gt;- Flag if equity &lt; -$10M | Review large losses |

## Cross-Item Validation Rules

### Consistency Checks

1. **Fee Consistency**
   - Initial franchise fee (Item 5) must appear in Item 7 initial investment
   - Royalty structure (Item 6) should align with revenue assumptions (Item 19)

2. **Outlet Validation**
   - Item 20 total outlets should be consistent with Item 19 sample sizes
   - State counts must sum to total outlet counts

3. **Financial Alignment**
   - Item 21 franchise revenue should align with Item 6 fee structures
   - Item 19 performance data should be reasonable given Item 21 financials

### Temporal Validation

1. **Document Dating**
   ```python
   # Rules
   - Issue date &lt;= Current date + 1 year
   - Amendment date &gt;= Issue date
   - Fiscal year end within fiscal year
   - Historical data shouldn&apos;t exceed 10 years
   ```

2. **Data Freshness**
   - Financial statements (Item 21) should be within 2 years
   - Outlet data (Item 20) should include current year or prior year
   - FPR data (Item 19) should be recent (within 3 years)

## Implementation Examples

### Basic Validation Function

```python
def validate_fdd_data(fdd_id: UUID) -&gt; ValidationReport:
    &quot;&quot;&quot;Comprehensive validation of FDD data&quot;&quot;&quot;
    
    errors = []
    warnings = []
    
    # Get all related data
    sections = get_fdd_sections(fdd_id)
    
    for section in sections:
        # Schema validation (automatic via Pydantic)
        try:
            validate_section_schema(section)
        except ValidationError as e:
            errors.append({
                &apos;section_id&apos;: section.id,
                &apos;type&apos;: &apos;SCHEMA&apos;,
                &apos;errors&apos;: e.errors()
            })
        
        # Business rules validation
        business_errors = validate_business_rules(section)
        errors.extend(business_errors)
        
        # Quality checks
        quality_issues = run_quality_checks(section)
        warnings.extend(quality_issues)
    
    # Cross-item validation
    cross_errors = validate_cross_items(sections)
    errors.extend(cross_errors)
    
    return ValidationReport(
        fdd_id=fdd_id,
        errors=errors,
        warnings=warnings,
        validated_at=datetime.now()
    )
```

### Custom Validators

```python
class OutletValidator:
    &quot;&quot;&quot;Specialized validation for Item 20 outlet data&quot;&quot;&quot;
    
    @staticmethod
    def validate_outlet_math(summary: OutletSummary) -&gt; Optional[str]:
        &quot;&quot;&quot;Validate outlet count mathematics&quot;&quot;&quot;
        calculated = (
            summary.count_start + 
            summary.opened - 
            summary.closed + 
            summary.transferred_in - 
            summary.transferred_out
        )
        
        if calculated != summary.count_end:
            return (
                f&quot;Outlet math error: {summary.count_start} + {summary.opened} &quot;
                f&quot;- {summary.closed} + {summary.transferred_in} &quot;
                f&quot;- {summary.transferred_out} = {calculated}, &quot;
                f&quot;but count_end = {summary.count_end}&quot;
            )
        return None
    
    @staticmethod
    def validate_state_totals(
        state_counts: List[StateCount],
        outlet_summary: List[OutletSummary]
    ) -&gt; Optional[str]:
        &quot;&quot;&quot;Validate state counts match outlet totals&quot;&quot;&quot;
        # Implementation as shown in pydantic_models.md
        pass
```

### Anomaly Detection

```python
class AnomalyDetector:
    &quot;&quot;&quot;Statistical anomaly detection for quality checks&quot;&quot;&quot;
    
    @staticmethod
    def check_fee_outliers(fees: List[OtherFee]) -&gt; List[Dict]:
        &quot;&quot;&quot;Identify unusual fee structures&quot;&quot;&quot;
        anomalies = []
        
        # Check royalty fees
        royalty_fees = [f for f in fees if &apos;royalty&apos; in f.fee_name.lower()]
        for fee in royalty_fees:
            if fee.amount_percentage and fee.amount_percentage &gt; 15:
                anomalies.append({
                    &apos;type&apos;: &apos;HIGH_ROYALTY&apos;,
                    &apos;severity&apos;: &apos;WARNING&apos;,
                    &apos;message&apos;: f&apos;Unusually high royalty: {fee.amount_percentage}%&apos;,
                    &apos;fee&apos;: fee
                })
        
        return anomalies
    
    @staticmethod
    def check_investment_ranges(items: List[InitialInvestment]) -&gt; List[Dict]:
        &quot;&quot;&quot;Identify unusual investment ranges&quot;&quot;&quot;
        anomalies = []
        
        for item in items:
            if item.low_cents and item.high_cents:
                ratio = item.high_cents / item.low_cents
                if ratio &gt; 10:
                    anomalies.append({
                        &apos;type&apos;: &apos;WIDE_RANGE&apos;,
                        &apos;severity&apos;: &apos;INFO&apos;,
                        &apos;message&apos;: f&apos;Very wide range for {item.category}: {ratio:.1f}x&apos;,
                        &apos;item&apos;: item
                    })
        
        return anomalies
```

## Validation Severity Levels

| Level | Description | Action Required |
|-------|-------------|-----------------|
| **ERROR** | Data violates core constraints | Block processing, require fix |
| **WARNING** | Unusual but possibly valid | Flag for review, allow processing |
| **INFO** | Notable patterns or outliers | Log for analysis, no action |

## Quality Metrics

### Data Completeness Score

```python
def calculate_completeness_score(fdd_id: UUID) -&gt; float:
    &quot;&quot;&quot;Calculate % of expected fields populated&quot;&quot;&quot;
    
    # Define expected fields by item
    expected = {
        5: [&apos;fee_name&apos;, &apos;amount_cents&apos;, &apos;due_at&apos;],
        6: [&apos;fee_name&apos;, &apos;frequency&apos;, &apos;calculation_basis&apos;],
        7: [&apos;category&apos;, &apos;low_cents&apos;, &apos;high_cents&apos;],
        # ... etc
    }
    
    total_expected = 0
    total_populated = 0
    
    # Calculate completion percentage
    # Implementation details...
    
    return (total_populated / total_expected) * 100
```

### Validation Dashboard Metrics

1. **Schema Validation Pass Rate**: % of records passing Pydantic validation
2. **Business Rule Compliance**: % of records meeting all business rules
3. **Data Quality Score**: Composite score based on completeness, accuracy, consistency
4. **Review Queue Size**: Number of records flagged for manual review
5. **Common Validation Errors**: Top 10 validation failures by frequency

## Best Practices

1. **Fail Fast**: Run schema validation before expensive operations
2. **Batch Validation**: Validate related records together for efficiency
3. **Clear Error Messages**: Include field name, expected value, actual value
4. **Validation Logging**: Track all validation attempts for auditing
5. **Progressive Enhancement**: Start with critical validations, add more over time
6. **Review Feedback Loop**: Update rules based on manual review findings

## Configuration

```yaml
# validation_config.yaml
validation:
  schema:
    enabled: true
    strict_mode: false
    
  business_rules:
    enabled: true
    outlet_math_check: true
    cross_item_validation: true
    
  quality_checks:
    enabled: true
    outlier_detection: true
    statistical_thresholds:
      royalty_fee_max: 15.0
      investment_range_ratio: 10.0
      
  severity_actions:
    ERROR: block_processing
    WARNING: flag_for_review
    INFO: log_only
```

---

**Note**: Validation rules should be regularly reviewed and updated based on:
- New FDD formats or requirements
- Patterns identified in manual reviews
- Business rule changes
- Data quality metrics</file><file path="docs/ARCHITECTURE.md"># FDD Pipeline Architecture

## System Overview

The FDD Pipeline is a distributed document processing system designed to handle the complete lifecycle of Franchise Disclosure Documents - from acquisition through state portals to structured data extraction and storage. The architecture emphasizes reliability, scalability, and data quality through multi-stage validation.

## High-Level Architecture

```mermaid
graph TB
    subgraph &quot;Data Sources&quot;
        MN[Minnesota Portal]
        WI[Wisconsin Portal]
    end
    
    subgraph &quot;Acquisition Layer&quot;
        SC[Scrapers&lt;br/&gt;Playwright/httpx]
        DD[Deduplication&lt;br/&gt;Engine]
    end
    
    subgraph &quot;Storage Layer&quot;
        GD[Google Drive&lt;br/&gt;PDF Storage]
        SB[(Supabase&lt;br/&gt;PostgreSQL)]
    end
    
    subgraph &quot;Processing Layer&quot;
        MU[MinerU Web API&lt;br/&gt;Layout Analysis]
        SEG[Document&lt;br/&gt;Segmentation]
        LLM[LLM Extraction&lt;br/&gt;Instructor]
    end
    
    subgraph &quot;Validation Layer&quot;
        SV[Schema&lt;br/&gt;Validation]
        BV[Business&lt;br/&gt;Validation]
        QC[Quality&lt;br/&gt;Control]
    end
    
    subgraph &quot;Access Layer&quot;
        API[FastAPI&lt;br/&gt;Internal]
        EF[Edge Functions&lt;br/&gt;Public]
    end
    
    subgraph &quot;Orchestration&quot;
        PF[Prefect&lt;br/&gt;Workflows]
    end
    
    MN --&gt; SC
    WI --&gt; SC
    SC --&gt; DD
    DD --&gt; GD
    DD --&gt; SB
    GD --&gt; MU
    MU --&gt; SEG
    SEG --&gt; LLM
    LLM --&gt; SV
    SV --&gt; BV
    BV --&gt; QC
    QC --&gt; SB
    SB --&gt; API
    SB --&gt; EF
    PF -.-&gt; SC
    PF -.-&gt; LLM
    PF -.-&gt; QC
```

## Component Architecture

### 1. Acquisition Layer

**Purpose**: Automated collection of FDD documents from state regulatory portals.

**Components**:
- **State-Specific Scrapers**: Custom Playwright scripts for each portal
- **Metadata Extractors**: Parse filing information during scrape
- **Deduplication Engine**: Prevent reprocessing of identical documents

**Key Design Decisions**:
- Playwright over Selenium for better reliability and performance
- State-specific scrapers rather than generic solution due to portal differences
- Immediate upload to Google Drive to avoid local storage

### 2. Storage Layer

**Purpose**: Centralized, cloud-based storage for documents and structured data.

**Components**:
- **Google Drive**: Document binary storage with folder hierarchy
- **Supabase PostgreSQL**: Structured data, metadata, and processing state

**Storage Strategy**:
```
Google Drive Structure:
/fdds/
  /{source}/           # MN, WI
    /{franchise_slug}/
      /{year}/
        /{document_type}/
          original.pdf
          /segments/
            section_00_intro.pdf
            section_01.pdf
            ...
            section_23.pdf
            section_24_appendix.pdf
```

### 3. Processing Layer

**Purpose**: Transform unstructured PDFs into structured, queryable data.

**Pipeline Stages**:
1. **Layout Analysis** (MinerU Web API)
   - Browser-based authentication workflow
   - Cloud-based PDF processing
   - Advanced table and structure detection
   - Returns structured JSON and markdown
   
2. **Section Identification**
   - Enhanced detection with Claude LLM (v2)
   - Rule-based header detection with fuzzy matching
   - Confidence scoring for ambiguous sections
   - Page range calculation with overlap handling
   
3. **Document Segmentation**
   - Split into 25 individual PDFs (Items 0-23 + appendix)
   - Maintain page number mapping
   - Handle multi-page sections
   
4. **LLM Extraction**
   - Section-specific prompts loaded from YAML
   - Structured output via Instructor framework
   - Multi-model routing based on complexity
   - Automatic retry with validation

### 4. Validation Layer

**Purpose**: Ensure data quality and consistency before storage.

**Validation Tiers**:

| Tier | Description | Examples | Action on Failure |
|------|-------------|----------|-------------------|
| Schema | Pydantic model validation | Required fields, type checking | Retry extraction |
| Business | Domain-specific rules | Totals matching, date logic | Flag for review |
| Quality | Completeness checks | Missing sections, OCR quality | Log warning |

### 5. Data Model

**Core Entities**:

```mermaid
erDiagram
    franchisors ||--o{ fdds : has
    fdds ||--o{ fdd_sections : contains
    fdd_sections ||--o| item5_initial_fees : extracts
    fdd_sections ||--o| item6_other_fees : extracts
    fdd_sections ||--o| item7_initial_investment : extracts
    fdd_sections ||--o| item19_fpr : extracts
    fdd_sections ||--o| item20_outlet_summary : extracts
    fdd_sections ||--o| item21_financials : extracts
    fdd_sections ||--o| fdd_item_json : stores
    
    franchisors {
        uuid id PK
        text canonical_name
        text parent_company
        text website
    }
    
    fdds {
        uuid id PK
        uuid franchise_id FK
        date issue_date
        text document_type
        text drive_path
        bool superseded
    }
    
    fdd_sections {
        uuid id PK
        uuid fdd_id FK
        int item_no
        int start_page
        int end_page
        text extraction_status
    }
```

### 6. Orchestration Layer

**Purpose**: Coordinate pipeline execution and handle failures.

**Prefect Flows**:
```python
# High-level flow structure
@flow
def process_state_fdds(state: str):
    # Acquisition
    new_docs = scrape_state_portal(state)
    
    # Deduplication
    unique_docs = deduplicate(new_docs)
    
    # Processing (mapped for parallelism)
    segmented = segment_documents.map(unique_docs)
    extracted = extract_sections.map(segmented)
    
    # Validation &amp; Storage
    validated = validate_data.map(extracted)
    store_results(validated)
```

**Scheduling**:
- Weekly runs for each state portal
- Retry logic: 3 attempts with exponential backoff
- Email alerts on failure

## Technology Decisions

### Why MinerU Web API?

**Pros**:
- State-of-the-art PDF layout analysis
- No local GPU/compute requirements
- Handles complex table structures
- Consistent results across documents

**Cons**:
- Requires browser authentication
- Rate limits on free tier
- Network dependency

**Alternative Considered**: Local PDF processing
- More control but significantly worse accuracy

### Why Google Drive?

**Pros**:
- No storage limits for workspace accounts
- Built-in versioning and audit trail
- Simple API for remote-only operations
- Cost-effective for large PDFs

**Cons**:
- API rate limits require careful handling
- No native database features

**Alternative Considered**: S3
- Better for programmatic access but higher complexity

### Why Supabase?

**Pros**:
- PostgreSQL with batteries included
- Built-in auth and RLS
- Edge Functions for API layer
- Real-time subscriptions (future use)

**Cons**:
- Vendor lock-in for some features
- Limited to PostgreSQL

**Alternative Considered**: Raw PostgreSQL + custom API
- More control but significantly more infrastructure

### Why Prefect?

**Pros**:
- Python-native with decorators
- Excellent observability
- Dynamic task mapping
- Local and cloud deployment options

**Cons**:
- Relatively new (v2)
- Smaller community than Airflow

**Alternative Considered**: Airflow
- More mature but heavier and more complex

### LLM Strategy

**Model Selection Logic**:
```python
def select_model(section: int, complexity: str) -&gt; str:
    # Simple structured data (tables)
    if section in [5, 6, 7] and complexity == &quot;low&quot;:
        return &quot;ollama:phi3-mini&quot;  # Fast, local
    
    # Complex narratives
    elif section in [19, 21] or complexity == &quot;high&quot;:
        return &quot;gemini-pro-2.5&quot;  # Best accuracy
    
    # Default
    else:
        return &quot;ollama:llama3-8b&quot;  # Balanced
```

**Fallback Chain**:
1. Primary model (based on selection)
2. Secondary model (next tier up)
3. OpenAI GPT-4 (highest cost, best reliability)

## Security Model

### Authentication &amp; Authorization
- **Supabase RLS**: Row-level security for data access
- **Service Keys**: Separate keys for different components
- **API Keys**: Stored in environment, never in code

### Data Privacy
- **PII Handling**: No PII extracted or stored
- **Document Access**: Service account with minimal permissions
- **Audit Trail**: All operations logged with user/service identity

### Network Security
- **HTTPS Only**: All external communications encrypted
- **Private Endpoints**: Internal APIs not exposed publicly
- **Rate Limiting**: Implemented at Edge Function layer

## Scalability Considerations

### Horizontal Scaling
- **Scrapers**: Multiple Prefect agents can run in parallel
- **Processing**: Task mapping allows parallel document processing
- **LLM Calls**: Async operations with connection pooling

### Vertical Scaling
- **Database**: Supabase auto-scales with usage
- **Storage**: Google Drive has no practical limits
- **Compute**: Local Prefect agents can be upgraded as needed

### Bottlenecks &amp; Mitigation
1. **MinerU API Rate Limits**
   - Solution: Queue with rate limiting
   - Authentication caching for session reuse
   - Future: Premium tier or self-hosted option

2. **LLM API Costs**
   - Solution: Ollama for simple tasks
   - Model routing based on complexity
   - Future: Fine-tuned models

3. **Database Connections**
   - Solution: Connection pooling via Supabase
   - Batch operations where possible
   - Future: Read replicas

4. **Browser Automation Stability**
   - Solution: Retry logic with exponential backoff
   - Headless mode for production
   - Future: API-only authentication

## Monitoring &amp; Observability

### Metrics Tracked
- Document processing rate
- Section extraction success rate
- LLM token usage by model
- Validation failure reasons
- End-to-end latency

### Logging Strategy
```python
# Structured logging example
logger.info(&quot;section_extracted&quot;, 
    section_id=section.id,
    item_no=section.item_no,
    model_used=model_name,
    tokens_used=response.usage.total_tokens,
    extraction_time=elapsed_time
)
```

### Alerting Rules
- Pipeline failure (any critical error)
- Extraction success rate &lt; 95%
- Processing time &gt; 2x average
- API rate limit approaching

## Future Architecture Considerations

### Phase 2 Enhancements
- **Streaming Processing**: Process documents as they arrive
- **Multi-Region**: Replicate to other cloud regions
- **API Gateway**: Kong or similar for advanced routing

### Phase 3 Vision
- **ML Pipeline**: Custom models for extraction
- **Real-time Updates**: WebSocket subscriptions
- **Data Warehouse**: Dedicated analytics infrastructure

## Development &amp; Deployment

### Local Development
```bash
# Minimum setup
- Prefect Server (local)
- PostgreSQL (Docker)
- Ollama (for local LLMs)
- Google Drive (service account)
```

### Deployment Architecture
```mermaid
graph LR
    subgraph &quot;Local Infrastructure&quot;
        PA[Prefect Agent]
        PS[Prefect Server]
        OL[Ollama Server]
    end
    
    subgraph &quot;Cloud Services&quot;
        SB[Supabase]
        GD[Google Drive]
        GM[Gemini API]
        OA[OpenAI API]
    end
    
    PA --&gt; PS
    PA --&gt; OL
    PA --&gt; SB
    PA --&gt; GD
    PA --&gt; GM
    PA --&gt; OA
```

## Key Architecture Principles

1. **Idempotency**: All operations can be safely retried
2. **Observability**: Every action is logged and measurable
3. **Fail-Safe**: Graceful degradation with fallback options
4. **Modularity**: Components can be upgraded independently
5. **Cost-Aware**: Use cheapest option that meets requirements

---

For detailed component documentation, see the `/docs/01_architecture/` directory.</file><file path="docs/TECH_STACK.md"># Technology Stack

This document provides a comprehensive overview of all technologies, frameworks, and packages used in the FDD Pipeline project.

## Core Python Version
- **Python 3.11+** - Required for latest type hints and performance improvements

## Package Management
- **[uv](https://github.com/astral-sh/uv)** (latest) - Fast Python package installer and resolver
  - Replaces pip, pip-tools, pipx, poetry, pyenv, virtualenv
  - 10-100x faster than pip
  - Handles virtual environments natively

## Workflow Orchestration
- **[Prefect](https://www.prefect.io/)** (2.14+) - Modern workflow orchestration
  - Flow and task decorators for pipeline definition
  - Built-in retry logic and error handling
  - Local and cloud deployment options
  - Real-time monitoring dashboard

## Web Scraping &amp; Automation
- **[Playwright](https://playwright.dev/python/)** (1.40+) - Browser automation
  - Handles JavaScript-heavy sites
  - Built-in wait strategies
  - Headless and headed modes
  - Used for state portal scraping
  - MinerU Web API authentication
  - Chromium browser required: `playwright install chromium`
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** (4.12+) - HTML parsing
  - Simple API for navigating HTML
  - Works with requests/httpx responses
  - Used for parsing portal search results
- **[httpx](https://www.python-httpx.org/)** (0.25+) - Modern HTTP client
  - Async/sync support
  - Connection pooling
  - Better than requests for production use
  - Used for API calls and file downloads

## Data Processing &amp; Validation
- **[Pydantic](https://docs.pydantic.dev/)** (2.5+) - Data validation using Python type annotations
  - Settings management from environment
  - JSON schema generation
  - Automatic validation
  - Integration with FastAPI
- **[pandas](https://pandas.pydata.org/)** (2.1+) - Data manipulation
  - Table parsing from HTML
  - CSV/Excel operations
  - Data cleaning utilities

## Database &amp; Storage
- **[supabase-py](https://github.com/supabase/supabase-py)** (2.3+) - Supabase client
  - PostgreSQL access
  - Real-time subscriptions
  - Storage bucket operations
  - Edge function invocation
- **[SQLAlchemy](https://www.sqlalchemy.org/)** (2.0+) - SQL toolkit and ORM
  - Complex query building
  - Connection pooling
  - Migration support
- **[google-api-python-client](https://github.com/googleapis/google-api-python-client)** (2.100+) - Google APIs
  - Drive API for file operations
  - Service account authentication
  - Resumable uploads

## AI/ML Stack

### LLM Integration
- **[instructor](https://github.com/jxnl/instructor)** (1.2+) - Structured LLM outputs
  - Pydantic model integration
  - Automatic retries on validation failure
  - Multi-provider support
- **[google-generativeai](https://github.com/google/generative-ai-python)** (0.3+) - Gemini Pro access
  - Primary LLM for complex extractions
  - Native multimodal support
- **[openai](https://github.com/openai/openai-python)** (1.6+) - OpenAI API
  - Fallback LLM provider
  - GPT-4 for high-complexity tasks
- **[ollama](https://github.com/ollama/ollama-python)** (0.1+) - Local LLM integration
  - Privacy-preserving processing
  - Cost-effective for simple tasks

### Document Processing
- **MinerU Web API** (custom client) - Advanced PDF analysis
  - Cloud-based processing (no local GPU needed)
  - Browser-based authentication via Playwright
  - Table and layout detection
  - Returns structured JSON and markdown
  - Located in `src/MinerU/mineru_web_api.py`
- **[PyPDF2](https://pypdf2.readthedocs.io/)** (3.0+) - PDF manipulation
  - Page splitting for section extraction
  - Metadata extraction
  - Text extraction fallback
  - Used in `utils/pdf_extractor.py`
- **[fitz (PyMuPDF)](https://pymupdf.readthedocs.io/)** (1.23+) - Advanced PDF operations
  - Better text extraction than PyPDF2
  - Image extraction from PDFs
  - Page rendering capabilities

### Embeddings &amp; Similarity
- **[sentence-transformers](https://www.sbert.net/)** (2.2+) - Semantic embeddings
  - Franchise name similarity
  - Deduplication
  - MiniLM-L6-v2 model (384 dimensions)

## API Framework
- **[FastAPI](https://fastapi.tiangolo.com/)** (0.104+) - Modern web API framework
  - Automatic OpenAPI documentation
  - Pydantic integration
  - Async support
  - Dependency injection
- **[uvicorn](https://www.uvicorn.org/)** (0.24+) - ASGI server
  - Production-ready performance
  - Auto-reload in development

## Development Tools

### Code Quality
- **[black](https://black.readthedocs.io/)** (23.12+) - Code formatter
  - Zero-config formatting
  - Consistent style across team
- **[flake8](https://flake8.pycqa.org/)** (6.1+) - Linting
  - PEP 8 compliance
  - Complexity checking
  - Plugin ecosystem
- **[mypy](http://mypy-lang.org/)** (1.7+) - Static type checker
  - Catches type errors before runtime
  - Gradual typing support
  - IDE integration

### Testing
- **[pytest](https://docs.pytest.org/)** (7.4+) - Testing framework
  - Fixture system
  - Parametrized tests
  - Plugin architecture
- **[pytest-asyncio](https://github.com/pytest-dev/pytest-asyncio)** (0.21+) - Async test support
- **[pytest-cov](https://pytest-cov.readthedocs.io/)** (4.1+) - Coverage reporting

### Git Hooks
- **[pre-commit](https://pre-commit.com/)** (3.5+) - Git hook management
  - Runs formatters/linters before commit
  - Consistent code quality
  - Language-agnostic

## Utilities
- **[python-dotenv](https://github.com/theskumar/python-dotenv)** (1.0+) - Environment management
  - .env file loading
  - Development/production separation
- **[jinja2](https://jinja.palletsprojects.com/)** (3.1+) - Template engine
  - Prompt template rendering
  - Dynamic YAML generation
- **[pyyaml](https://pyyaml.org/)** (6.0+) - YAML parsing
  - Prompt template loading
  - Configuration files

## Monitoring &amp; Logging
- **[structlog](https://www.structlog.org/)** (23.2+) - Structured logging
  - JSON output
  - Context preservation
  - Performance
- **[python-json-logger](https://github.com/madzak/python-json-logger)** (2.0+) - JSON log formatting

## Additional Dependencies
- **[python-magic](https://github.com/ahupp/python-magic)** (0.4+) - File type detection
  - MIME type identification
  - Binary file validation
- **[Pillow](https://python-pillow.org/)** (10.0+) - Image processing
  - PDF page to image conversion
  - Image optimization for LLM processing
- **[tiktoken](https://github.com/openai/tiktoken)** (0.5+) - Token counting
  - OpenAI model token estimation
  - Context window management
- **[tenacity](https://tenacity.readthedocs.io/)** (8.2+) - Retry logic
  - Advanced retry strategies
  - Exponential backoff implementation

## Email &amp; Notifications
- **Built-in `smtplib`** - Email sending
  - SMTP/TLS support
  - HTML emails
  - Attachment support
  - Used for pipeline failure alerts

## Type Definitions
- **[types-requests](https://github.com/python/typeshed)** - Type stubs
- **[types-pyyaml](https://github.com/python/typeshed)** - Type stubs
- **[pandas-stubs](https://github.com/pandas-dev/pandas-stubs)** - Pandas type stubs

## Optional/Future Additions
- **[redis](https://github.com/redis/redis-py)** - Caching layer
- **[celery](https://docs.celeryproject.org/)** - Distributed task queue
- **[sentry-sdk](https://docs.sentry.io/platforms/python/)** - Error tracking
- **[prometheus-client](https://github.com/prometheus/client_python)** - Metrics export

## Development Environment

### Recommended IDE Extensions
- **VS Code**:
  - Python
  - Pylance
  - Black Formatter
  - GitLens
  - Thunder Client (API testing)

### System Requirements
- **Minimum**: 8GB RAM, 4 CPU cores
- **Recommended**: 16GB RAM, 8 CPU cores
- **GPU**: Optional, for local LLM inference (8GB VRAM minimum)

## Version Pinning Strategy
- Exact versions in `requirements.txt` for production
- Flexible versions in `pyproject.toml` for development
- Monthly dependency updates with testing
- Security patches applied immediately

## Installation Example

```bash
# Using uv (recommended)
uv pip install -r requirements.txt

# Development dependencies
uv pip install -r requirements-dev.txt

# Or install from pyproject.toml
uv pip install -e &quot;.[dev]&quot;
```

## Package Organization

```toml
# pyproject.toml structure
[project]
name = &quot;fdd-pipeline&quot;
dependencies = [
    &quot;prefect&gt;=2.14&quot;,
    &quot;pydantic&gt;=2.5&quot;,
    &quot;fastapi&gt;=0.104&quot;,
    # ... core deps
]

[project.optional-dependencies]
dev = [
    &quot;pytest&gt;=7.4&quot;,
    &quot;black&gt;=23.12&quot;,
    &quot;mypy&gt;=1.7&quot;,
    # ... dev deps
]
```

## Security Considerations
- All packages verified through PyPI
- Dependency scanning via GitHub Dependabot
- No packages with known CVEs
- Regular security audits with `pip-audit`

---

For specific version constraints and the complete dependency tree, see `requirements.txt` and `requirements.lock`.</file></files></repomix>